# OrgOps Process Definition: Pulse Survey Process
# Schema: org_ops_definition_schema.json v1.1.0

id: pd-009
name: Pulse Survey Process
slug: pulse-survey-process
track: org_ops
status: ready

contributes_to:
  - OrgOps.Feedback Mechanisms.pulse-surveys

maturity:
  current_tier: 1
  
  tier_1_basic:
    description: "Simple periodic check-in on team sentiment"
    includes:
      - 3-5 question survey
      - Quarterly frequency
      - Basic results sharing
    effort: "1-2 hours to set up, 30 min per cycle"
    
  tier_2_intermediate:
    description: "Regular pulse with trend tracking"
    adds:
      - Monthly or bi-weekly frequency
      - Trend analysis over time
      - Benchmarking (internal or external)
      - Action planning based on results
      - Anonymous comments
    effort: "Half day to set up, 1-2 hours per cycle"
    
  tier_3_advanced:
    description: "Continuous listening with analytics"
    adds:
      - Real-time dashboards
      - Team-level breakdowns
      - Sentiment analysis
      - Integration with HRIS
      - Automated alerts for concerning trends
    effort: "1-2 weeks to implement, ongoing analysis"

definition:
  purpose: |
    Annual surveys are too infrequent to catch issues early. By the time problems 
    surface, damage is done. Pulse surveys provide regular temperature checks so 
    leaders can respond before small issues become big problems.
    
  outcome: |
    Leadership has current understanding of team sentiment. Trends are visible 
    over time. Action is taken on concerning patterns before they escalate.
    
  owner: HR Lead or People Operations
  
  inputs:
    - name: Survey questions
      description: What to ask (engagement, satisfaction, specific topics)
      source: HR, leadership priorities
    - name: Distribution list
      description: Who receives the survey
      source: HRIS, team lists
    - name: Previous results
      description: Prior pulse data for comparison
      source: Survey tool history
      
  outputs:
    - name: Survey results summary
      description: Aggregated responses with trends
      consumer: Leadership, managers
    - name: Action items
      description: Specific improvements based on feedback
      consumer: Responsible owners
    - name: Trend report
      description: How metrics are changing over time
      consumer: Leadership
      
  steps:
    - step: 1
      action: Prepare survey questions (reuse or update)
      responsible: HR Lead
      duration: "15 minutes"
      deliverable: Survey ready
      
    - step: 2
      action: Send survey to all employees
      responsible: HR Lead
      duration: "10 minutes"
      tools:
        - Survey tool (Officevibe, Culture Amp, Google Forms)
      deliverable: Survey distributed
      
    - step: 3
      action: Allow response window (3-5 business days)
      responsible: Employees
      duration: "3-5 days"
      deliverable: Responses collected
      
    - step: 4
      action: Analyze results and identify themes
      responsible: HR Lead
      duration: "30-60 minutes"
      deliverable: Results summary
      
    - step: 5
      action: Share results with leadership
      responsible: HR Lead
      duration: "30 minutes"
      deliverable: Leadership briefed
      
    - step: 6
      action: Communicate key findings to team
      responsible: Leadership
      duration: "15 minutes"
      deliverable: Team informed
      
    - step: 7
      action: Identify and assign actions for improvement
      responsible: Leadership
      duration: "30 minutes"
      deliverable: Action items assigned
      
  cadence:
    frequency: quarterly
    trigger: Calendar cadence (can increase to monthly)
    duration: "1-2 weeks from send to action"
    
  participants:
    - role: HR Lead
      responsibility: Design survey, analyze results, coordinate
      time_commitment: "2-3 hours per cycle"
    - role: Employees
      responsibility: Complete survey honestly
      time_commitment: "5-10 minutes"
    - role: Leadership
      responsibility: Review results, take action, communicate
      time_commitment: "1-2 hours per cycle"
      
  success_criteria:
    - Response rate above 70%
    - Results analyzed within 1 week of close
    - Key findings communicated to team
    - At least one action taken per cycle
    - Trends visible over time

operational_context:
  prerequisites:
    - Survey tool set up
    - Leadership commitment to act on feedback
    - Trust that responses are confidential
    
  common_blockers:
    - "Low response rate → Keep surveys short, remind, show past actions"
    - "No action on feedback → Close the loop, communicate changes"
    - "Survey fatigue → Keep frequency reasonable, questions focused"
    
  metrics:
    - name: Response rate
      description: Percentage of employees completing survey
      target: "> 70%"
      tier: 1
    - name: eNPS (Employee Net Promoter Score)
      description: Would you recommend working here?
      target: "> 30 is good, > 50 is excellent"
      tier: 2
    - name: Action completion rate
      description: Percentage of identified actions completed
      target: "> 80%"
      tier: 2

tools_ecosystem:
  recommended:
    - category: Employee Engagement Platforms
      examples:
        - Officevibe
        - Culture Amp
        - Lattice
        - 15Five
      purpose: Automated surveys, analytics, benchmarks
    - category: Simple (Tier 1)
      examples:
        - Google Forms
        - Typeform
        - Microsoft Forms
      purpose: Basic survey collection

related_definitions:
  - id: pd-006
    relationship: enables
    description: "Pulse data can inform performance conversations"

adaptation:
  customize:
    - Question set based on current priorities
    - Frequency (quarterly, monthly, weekly)
    - Anonymity level
  preserve:
    - Consistency of core questions for trending
    - Action follow-through
    - Communication of results
  common_variations:
    - "Weekly: 1-2 questions, very short"
    - "Monthly: 3-5 questions, quick check"
    - "Quarterly: 10-15 questions, deeper dive"

# Sample Questions by Category
sample_questions:
  engagement:
    - "How motivated are you to do your best work? (1-10)"
    - "Would you recommend this company as a great place to work? (0-10 NPS)"
    
  satisfaction:
    - "How satisfied are you with your role? (1-5)"
    - "Do you feel your work is recognized? (1-5)"
    
  manager:
    - "Do you feel supported by your manager? (1-5)"
    - "Is your manager accessible when you need them? (1-5)"
    
  growth:
    - "Do you have opportunities to learn and grow? (1-5)"
    - "Is your career path clear? (1-5)"
    
  open_ended:
    - "What's one thing we could do to improve?"
    - "What's working well that we should keep doing?"

# ============================================================
# PRACTITIONER SCENARIOS
# ============================================================
practitioner_scenarios:
  - id: ps-009-001
    name: "First Pulse Survey After Layoffs"
    practitioner: Head of People at 40-person company
    organization_context: |
      Company went through 20% reduction in force 6 weeks ago. Morale is 
      unknown—some remaining employees are relieved, others are anxious. 
      Leadership wants to understand sentiment but worried that a survey 
      will surface negativity or seem tone-deaf. No pulse survey has ever 
      been done; annual survey was 8 months ago.
    situation: |
      CEO asks: "How are people really feeling?" Leadership is operating on 
      hallway conversations and vibes. Some managers say their teams are fine, 
      others report anxiety. No systematic way to know. Head of People 
      suggests pulse survey but CEO is worried about the results.
    trigger: |
      Two valued employees resign in the same week, citing "uncertainty about 
      the future." CEO says: "We need to know what's happening. Let's do a 
      pulse check—I'd rather know the truth than be surprised by more exits."
    actions:
      - "Day 1: Decided on anonymous 8-question survey (short, manageable)"
      - "Day 1: Included questions about confidence in leadership, workload, and what they need"
      - "Day 2: CEO sent introduction email explaining why we're asking and committing to share results"
      - "Day 2: Opened survey for 4 business days (short window to reduce overthinking)"
      - "Week 1: 85% response rate—people wanted to be heard"
      - "Week 1: Results were mixed—high confidence in product, low confidence in 'knowing what's next'"
      - "Week 2: Leadership team reviewed results, identified 3 actionable themes"
      - "Week 2: CEO all-hands to share results openly, including uncomfortable ones"
      - "Week 2: Announced 3 concrete actions: more transparency on company metrics, workload review, skip-levels"
      - "Month 2: Follow-up pulse on same questions—scores improved on 6 of 8 items"
    outcome: |
      Survey surfaced what leadership suspected but couldn't prove: people 
      wanted transparency, not just reassurance. The 3 actions (metrics 
      sharing, workload review, skip-levels) addressed real concerns. Follow-up 
      pulse showed improvement. More importantly, the act of asking and 
      sharing results rebuilt some trust damaged during layoffs.
    lessons:
      - "Pulse surveys during difficult times can rebuild trust if handled transparently"
      - "CEO introduction and commitment to share results increases response and impact"
      - "Short surveys (8-10 questions) get higher response rates"
      - "Sharing uncomfortable results builds more trust than sanitizing them"
      - "Follow-up pulse on same questions shows whether actions worked"

  - id: ps-009-002
    name: "Discovering Manager Problem Through Pulse Data"
    practitioner: VP People at 80-person company
    organization_context: |
      Monthly pulse surveys established for 1 year. Overall scores are 
      healthy (7.5+ out of 10 on key metrics). But one team's scores have 
      been declining for 3 consecutive months. The manager of that team 
      seems competent and is well-liked by peers. No direct complaints 
      to HR. Something is off but no one knows what.
    situation: |
      Pulse data shows one team (8 people) dropped from 8.2 to 5.8 on 
      "manager support" over 3 months. Other teams stable. VP People 
      meets with manager—they seem surprised and defensive. Employees 
      on the team are not coming forward directly. Need to understand 
      what's happening without creating a witch hunt.
    trigger: |
      Third consecutive month of decline. VP People shows data to CEO: 
      "Something is happening on this team. We need to dig deeper." CEO 
      agrees to investigate carefully without assuming the worst.
    actions:
      - "Week 1: VP People had informal 1:1s with 3 team members (framed as 'checking in on the team')"
      - "Week 1: Pattern emerged: manager had become micromanaging after a missed project deadline"
      - "Week 1: Employees felt unable to make decisions, were afraid of making mistakes"
      - "Week 2: VP People met with manager to share observations (not accusations)"
      - "Week 2: Manager admitted stress after deadline miss, overcompensated with control"
      - "Week 2: Agreed on coaching plan: delegation targets, reduced check-in frequency"
      - "Week 3: Manager communicated to team: 'I hear I've been too in the weeds. Working on it.'"
      - "Month 2: Next pulse showed improvement (6.4 → 7.2)"
      - "Month 3: Continued improvement (7.2 → 7.8)"
      - "Month 4: Team scores back to baseline, manager behavior sustainably changed"
    outcome: |
      Pulse data surfaced a problem no one was reporting directly. The 
      manager was not a bad person—they were stressed and overcompensating. 
      Coaching rather than punishment preserved the relationship and 
      fixed the behavior. Team felt heard even though they never explicitly 
      complained. Pattern established: pulse data triggers investigation, 
      not punishment.
    lessons:
      - "Team-level trends are more revealing than company averages"
      - "Declining scores over time are stronger signals than single low scores"
      - "Investigation should be curious, not accusatory—assume stress before malice"
      - "Managers often don't realize their behavior has changed under pressure"
      - "Pulse data creates accountability without requiring direct complaints"

  - id: ps-009-003
    name: "Building Pulse Survey Credibility from Scratch"
    practitioner: HR Manager at 25-person startup
    organization_context: |
      Fast-growing startup, never had formal employee feedback. Culture is 
      "we talk openly." But some employees are quieter, and not everyone 
      feels safe raising concerns. Founders are skeptical of "corporate HR 
      stuff" but recognize they can't talk to everyone as the team grows.
    situation: |
      HR Manager proposes pulse surveys. CEO says: "We have an open door 
      policy. If people have issues, they tell us." HR Manager explains 
      that not everyone feels comfortable with direct feedback to founders. 
      COO is more receptive: "I'd like to know what people think without 
      them having to say it to my face."
    trigger: |
      Exit interview from departing employee reveals issues that were 
      never raised during employment. COO says: "We could have fixed 
      this if we knew. Let's try the pulse survey idea."
    actions:
      - "Week 1: Designed minimal survey: 5 questions, anonymous, 2-minute completion"
      - "Week 1: Chose simple tool (Google Forms) to avoid 'corporate' feel"
      - "Week 2: CEO introduced in team meeting: 'I want to hear from everyone, even if you're not comfortable telling me directly'"
      - "Week 2: First survey launched—62% response rate (low for anonymous survey)"
      - "Week 2: Results shared at next all-hands: 'Here's what you said, here's what we heard'"
      - "Week 3: Took one action based on feedback (implemented no-meeting Fridays)"
      - "Month 2: Second survey—response rate increased to 78%"
      - "Month 2: Publicly credited pulse feedback for the no-meeting policy"
      - "Month 3: Third survey—85% response, richer open-ended comments"
      - "Month 4: Team started asking 'When's the next pulse?' (now expected, not resisted)"
    outcome: |
      Skepticism about "corporate HR stuff" disappeared when team saw 
      results lead to action. Response rates climbed as credibility built. 
      The simple format (5 questions, Google Forms) avoided over-engineering. 
      CEO became a convert: "This gives me signal I couldn't get otherwise." 
      Pulse surveys became part of the operating rhythm.
    lessons:
      - "Start minimal—5 questions and a simple tool beats elaborate systems"
      - "First surveys will have lower response; credibility builds over time"
      - "Visible action from feedback is the single biggest driver of participation"
      - "Leadership introduction and commitment matters—don't launch silently"
      - "Skeptics become converts when they see the value in practice"

# ============================================================
# RISKS
# ============================================================
risks:
  execution_risks:
    - risk: Low response rates
      likelihood: medium
      impact: medium
      mitigation: "Keep surveys short, leadership endorsement, show action from previous feedback"
    
    - risk: Survey fatigue from over-surveying
      likelihood: medium
      impact: medium
      mitigation: "Monthly or quarterly cadence maximum, keep surveys under 10 questions"
    
    - risk: Misleading data from small samples
      likelihood: low
      impact: medium
      mitigation: "Don't draw conclusions from small teams (<5), focus on trends over single data points"
    
    - risk: Anonymity breached or perceived as breached
      likelihood: low
      impact: high
      mitigation: "Use truly anonymous tools, don't share team-level data for teams <5, communicate privacy clearly"
    
    - risk: No action taken on results
      likelihood: high
      impact: high
      mitigation: "Commit to 1-3 actions per survey cycle, publicly credit feedback when making changes"

  organizational_risks:
    - risk: Results weaponized against managers
      impact: "Managers fear feedback, become defensive, culture becomes punitive not developmental"
      prevention: "Frame as development not punishment, investigate before acting, coaching before consequences"
    
    - risk: Surveys replace real conversations
      impact: "Leaders rely on data instead of talking to people, lose human connection"
      prevention: "Surveys supplement, don't replace, direct conversation. Follow up surveys with 1:1s."
    
    - risk: Gaming or strategic responses
      impact: "Data becomes unreliable, lose signal value"
      prevention: "Emphasize anonymity, focus on trends not absolute numbers, triangulate with other signals"

# ============================================================
# DOMAIN CONTEXT
# ============================================================
domain_context:
  industry_landscape: |
    Annual engagement surveys have given way to more frequent pulse surveys, 
    reflecting the need for real-time organizational awareness. The shift 
    accelerated during COVID-19 when organizations needed to understand 
    rapidly changing employee sentiment.
    
    Modern pulse survey tools (Culture Amp, Lattice, Officevibe, Peakon) 
    offer sophisticated analytics, benchmarking, and action planning. But 
    the tool matters less than the discipline of asking, sharing, and acting.
    
    The best organizations treat pulse surveys as conversation starters, 
    not conversation replacements. Data surfaces issues; humans resolve them.

  best_practices:
    - practice: "Keep it short (5-10 questions)"
      description: "Short surveys get higher response rates and more thoughtful answers"
      evidence: "Response rates drop 20% for every 5 additional questions beyond 10"
    
    - practice: "Consistent core questions for trending"
      description: "Ask the same 3-5 questions every time to track trends"
      evidence: "Trend data is more actionable than point-in-time scores"
    
    - practice: "Share results transparently"
      description: "Share what you learned, including uncomfortable findings"
      evidence: "Transparent sharing increases response rates 25% on subsequent surveys"
    
    - practice: "Commit to action and follow through"
      description: "Take at least one visible action from each survey cycle"
      evidence: "Action follow-through is the #1 predictor of sustained participation"

  anti_patterns:
    - anti_pattern: "Survey and disappear"
      description: "Collecting data without sharing results or taking action"
      consequence: "Response rates decline, employees feel unheard, surveys become pointless"
    
    - anti_pattern: "Over-surveying"
      description: "Weekly surveys that create fatigue and resentment"
      consequence: "Response quality declines, employees tune out, signal value lost"
    
    - anti_pattern: "Hunting for blame"
      description: "Using pulse data to punish managers rather than develop them"
      consequence: "Managers fear feedback, become defensive, culture becomes punitive"
    
    - anti_pattern: "Company-level averages only"
      description: "Ignoring team-level variation in pursuit of headline metrics"
      consequence: "Problems hide in averages, struggling teams undetected"

  benchmarks:
    - metric: "Response rate"
      top_quartile: ">85%"
      median: "70-85%"
      laggard: "<50%"
    
    - metric: "Action rate"
      top_quartile: "1-3 visible actions per cycle"
      median: "Occasional action"
      laggard: "No visible action"
    
    - metric: "Survey cadence"
      top_quartile: "Monthly with quarterly deep-dives"
      median: "Quarterly"
      laggard: "Annual or sporadic"
    
    - metric: "Trend tracking"
      top_quartile: "Core questions consistent, trends visible over 12+ months"
      median: "Some consistency"
      laggard: "Different questions each time"

  maturity_indicators:
    tier_1_signals:
      - "Regular pulse cadence established"
      - "Results shared with team"
      - "Leadership reviews data"
      - "Some action taken"
    
    tier_2_signals:
      - "Trend tracking over time"
      - "Team-level analysis"
      - "Action planning integrated with results"
      - "Response rates consistently high"
    
    tier_3_signals:
      - "Predictive analytics (leading indicators of turnover, etc.)"
      - "Integration with other people data"
      - "Manager coaching based on team feedback"
      - "Pulse drives strategic people decisions"

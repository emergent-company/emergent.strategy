# Document Chunking Implementation Summary

**Date**: October 20, 2025  
**PR**: Extraction Document Chunking with Overlap  
**Status**: âœ… Complete and Ready for Testing

## What Was Done

### Problem
User reported extraction job failing with: `Invalid JSON response from LLM (finish_reason: MAX_TOKENS)`

### Root Cause
Vertex AI provider was sending entire document in a single LLM call, causing token limit to be exceeded on large documents.

### Solution
Implemented document chunking with overlap in `vertex-ai.provider.ts`, matching the approach used in `langchain-gemini.provider.ts`.

## Changes Made

### 1. Added Document Chunking
- **File**: `apps/server/src/modules/extraction-jobs/llm/vertex-ai.provider.ts`
- **New Import**: `RecursiveCharacterTextSplitter` from LangChain
- **New Methods**:
  - `splitDocumentIntoChunks()` - Splits document with smart separators and overlap
  - `extractEntitiesForType()` - Processes single chunk for single entity type
  - `deduplicateEntities()` - Removes duplicate entities by name (case-insensitive)

### 2. Refactored Main Extraction Logic
- Changed from single LLM call to chunk-by-chunk processing
- Process each entity type separately for each chunk
- Collect detailed debug info for each call
- Aggregate token usage across all calls
- Return enhanced debug data in `raw_response`

### 3. Key Features

#### Smart Chunking
- Default: 100KB chunks (~25,000 tokens)
- Configurable via environment variables
- Uses intelligent separators (paragraph â†’ sentence â†’ word boundaries)

#### Overlap Between Chunks
- Default: 2KB overlap (~500 tokens)
- Captures entities spanning chunk boundaries
- Prevents loss of context at boundaries

#### Deduplication
- Case-insensitive name matching
- Keeps entity with highest confidence
- Applied per entity type

#### Enhanced Observability
- Per-call metrics (timing, tokens, entities found)
- Total aggregated statistics
- Detailed error information
- Visible in UI extraction logs

## Configuration

### Environment Variables
```env
EXTRACTION_CHUNK_SIZE=100000      # Characters per chunk (default: 100KB)
EXTRACTION_CHUNK_OVERLAP=2000     # Character overlap (default: 2KB)
```

### Tuning Guidelines
- **Small documents** (< 30KB): Single chunk, no overhead
- **Medium documents** (30-100KB): 1-2 chunks
- **Large documents** (100KB+): Multiple chunks with deduplication

## Testing Instructions

### 1. Test Small Document (Baseline)
Upload document < 100KB, should process in single chunk.

**Expected Logs**:
```
Document NOT split, using single chunk
Extracting 3 types: Location, Organization, Person
Extracted 42 total entities in 3456ms, tokens: 14500
```

### 2. Test Large Document (Chunking)
Upload document > 100KB (the one that failed before).

**Expected Logs**:
```
Splitting document (150000 chars) into chunks (size: 100000, overlap: 2000)
Created 2 chunks
Document split into 2 chunks for processing
Extracting 3 types: Location, Organization, Person
Extracted 15 unique Location entities (20 total before deduplication) in 4567ms
Extracted 42 total entities across 3 types in 15890ms, tokens: 87000
```

### 3. Verify in UI Extraction Logs
Open extraction logs for the job, check `output_data` field should contain:
```json
{
  "llm_calls": [
    {
      "type": "Location",
      "chunk_index": 0,
      "chunk_count": 2,
      "entities_found": 10,
      "duration_ms": 2345,
      "status": "success",
      "usage": { "total_tokens": 14500 }
    },
    // ... more calls
  ],
  "total_duration_ms": 15890,
  "total_entities": 42,
  "types_processed": 3,
  "chunks_processed": 2
}
```

### 4. Verify No MAX_TOKENS Errors
Previously failing document should now succeed without `finish_reason: MAX_TOKENS` errors.

## Performance Characteristics

### Token Usage
- **Before**: 1 failed call (~25,000 tokens wasted)
- **After**: Multiple successful calls (~75,000 tokens total for 100KB doc)
- **Trade-off**: More tokens used, but extraction succeeds

### Processing Time
- **Before**: Fast failure (~2-3 seconds)
- **After**: Longer processing (~15-20 seconds for large docs)
- **Trade-off**: Slower but successful

### Cost Implications
For 100KB document with 3 entity types:
- 2 chunks Ã— 3 types = **6 LLM calls**
- Input: ~75,000 tokens
- Output: ~12,000 tokens
- Total: ~87,000 tokens vs. 0 entities extracted before

## Monitoring

### Key Metrics to Watch
1. **Chunk count distribution** - How many documents need chunking?
2. **Duplicate rate** - How many entities found in multiple chunks?
3. **Token usage per document size** - Cost scaling
4. **Processing time per document size** - Performance scaling

### Database Queries
```sql
-- Check chunk processing stats
SELECT 
    output_data->>'chunks_processed' as chunks,
    COUNT(*) as job_count,
    AVG((output_data->>'total_entities')::int) as avg_entities
FROM kb.object_extraction_logs
WHERE operation_name = 'extract_entities'
    AND status = 'success'
GROUP BY chunks;
```

## Rollback Plan

If issues occur:
1. Git revert the commit
2. Restart server
3. User can temporarily switch to `langchain-gemini` provider (already has chunking)

## Documentation

Created comprehensive documentation:

1. **`EXTRACTION_CHUNKING_IMPLEMENTATION.md`** (600+ lines)
   - Architecture and code changes
   - Configuration and tuning
   - Testing procedures
   - Monitoring queries
   - Future enhancements

2. **Updated `EXTRACTION_JOB_43E7FED5_INVESTIGATION.md`**
   - Marked as RESOLVED
   - Added reference to chunking implementation

## Next Steps

1. **Immediate**: Test with the document that previously failed (job 43e7fed5)
2. **Short-term**: Monitor chunk statistics and token usage in production
3. **Medium-term**: Tune chunk size/overlap based on real data
4. **Long-term**: Consider parallel chunk processing for better performance

## Files Changed

```
Modified:
  apps/server/src/modules/extraction-jobs/llm/vertex-ai.provider.ts

Created:
  docs/EXTRACTION_CHUNKING_IMPLEMENTATION.md
  docs/EXTRACTION_CHUNKING_SUMMARY.md

Updated:
  docs/EXTRACTION_JOB_43E7FED5_INVESTIGATION.md
```

## Build Status

âœ… TypeScript compilation successful  
âœ… All imports resolved  
âœ… No linting errors

## Commit Message Suggestion

```
feat(extraction): Add document chunking with overlap to Vertex AI provider

- Split large documents into configurable chunks (default 100KB)
- Add overlap between chunks to capture spanning entities (default 2KB)
- Deduplicate entities by name across chunks (case-insensitive)
- Process each entity type separately per chunk
- Aggregate token usage and timing across all calls
- Enhanced debug data with per-call metrics
- Configurable via EXTRACTION_CHUNK_SIZE and EXTRACTION_CHUNK_OVERLAP env vars

Fixes: MAX_TOKENS error on large documents
References: #43e7fed5 (example failed job)
Documentation: docs/EXTRACTION_CHUNKING_IMPLEMENTATION.md
```

---

**Ready to test!** ðŸš€

Try re-running the extraction that failed before. It should now succeed and you'll see:
- Number of chunks created
- Entities extracted per chunk
- Deduplication statistics
- Total token usage
- Detailed per-call debug info in logs

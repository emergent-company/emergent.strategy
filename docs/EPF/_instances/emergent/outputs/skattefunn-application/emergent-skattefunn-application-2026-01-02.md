# SkatteFUNN - Tax Deduction Scheme Application

**Application Date:** 2026-01-02  
**Status:** Draft  
**Project Period:** 2026-01-01 to 2028-06-30 (30 months)

---

## Section 1: Project Owner and Roles

### 1.1 Project Owner

The Project Owner is responsible for running the project in accordance with the contract documents.

| Organisation Name | Organisation Number | Manager |
| --- | --- | --- |
| Outblocks AS | [To be provided] | Nikolai Fasting |

### 1.2 Roles in the Project

Three mandatory roles required:

| Name | Role | E-mail | Phone | Access Rights |
| --- | --- | --- | --- | --- |
| Nikolai Fasting | **Creator of Application** | [To be provided] | [To be provided] | Delete, Submit, Edit, Read, Withdraw, ChangeAccess |
| Nikolai Fasting | **Organisation Representative** | [To be provided] | [To be provided] | Edit, Read, Approve |
| Nikolai Fasting | **Project Leader** | [To be provided] | [To be provided] | Delete, Submit, Edit, Read, Withdraw, ChangeAccess |

---

## Section 2: About the Project

### 2.1 Project Title

**Title (English):** Emergent: AI-Powered Knowledge Infrastructure for Organizations  
*[Max 100 characters: 68/100]*

**Title (Norwegian):** Emergent: AI-drevet kunnskapsinfrastruktur for organisasjoner  
*[Max 100 characters: 66/100]*

**Short Name:** Emergent-KI  
*[Max 60 characters: 12/60]*

### 2.2 Scientific Classification

**Subject Area:** 2 - Engineering and Technology  
**Subject Group:** 2.1 - Data and Information Technology  
**Subject Discipline:** 2.1.1 - Computer and Information Sciences

### 2.3 Additional Information

**Area of Use:** Product, process, or service development  
*(Industry where project results will be applied)*

**Continuation of Previous Project:** No  
**Other Companies Applying for This Project:** No

---

## Section 3: Background and Company Activities

### 3.1 Company Activities

Outblocks AS develops AI-powered software tools and platforms for knowledge management, document intelligence, and organizational understanding. Our primary focus is building infrastructure that makes organizational information accessible, connected, and actionable for both human knowledge workers and AI agents.

We specialize in combining natural language processing, graph database engineering, and AI integration to transform unstructured data (documents, code repositories, meeting transcripts, web content) into structured, queryable knowledge representations. Our products serve software development teams, product organizations, and knowledge-intensive businesses requiring systematic approaches to information management and strategic planning.

The company operates at the intersection of enterprise software, AI tooling, and developer infrastructure. Our target market includes mid-sized technology companies (50-500 employees) and innovation-focused enterprises seeking to leverage AI for knowledge work without vendor lock-in or proprietary closed systems.

*[Max 2000 characters: 928/2000]*

*Describe your products/services, markets, and company stage (startup/scale-up/established).*

### 3.2 Project Background

This project addresses critical challenges in knowledge management for the AI era. Organizations accumulate vast amounts of unstructured information across documents, codebases, wikis, and collaboration tools, but lack infrastructure enabling AI agents to reason over this knowledge reliably. Existing solutions either provide document storage without semantic understanding (Notion, Confluence) or AI capabilities without grounded knowledge foundations (ChatGPT, Claude).

Emergent represents our strategic entry into production-scale knowledge infrastructure. While we have validated core technical concepts through prototypes—graph-based knowledge representation, LLM-powered extraction, vector similarity search—scaling to production requirements introduces substantial technical uncertainties requiring systematic R&D.

The project is critical for our company's development because it establishes our position in the emerging market for AI-native knowledge infrastructure. Success means organizations can deploy Emergent as their knowledge backbone, replacing fragmented tools with unified semantic layers that support both human and AI workflows. This project also demonstrates our Emergent Product Framework (EPF) methodology, using it to build Emergent itself—validating both product and process simultaneously.

*[Max 2000 characters: 1127/2000]*

*Explain why this project is important for your company's development.*

---

## Section 4: Primary Objective and Innovation

### 4.1 Primary Objective

Develop and validate production-ready AI-powered knowledge infrastructure enabling organizations to transform unstructured information into queryable knowledge graphs accessible to AI agents. Demonstrate that combining graph database technology, vector embeddings, and AI-native protocols achieves: (1) 10,000+ knowledge objects with sub-200ms query latency under concurrent access, (2) multi-format document extraction (PDF/Markdown/code) with 95%+ accuracy, (3) AI agent integration through Model Context Protocol with measurable task improvement, and (4) systematic methodology frameworks (EPF, OpenSpec) for strategic alignment. Project progresses technologies from TRL 3-4 (component validation) to TRL 5-6 (system prototype in relevant environment), de-risking technical uncertainties blocking production deployment.

*[Max 1000 characters: 735/1000]*

*State concrete, verifiable goals and describe what new or improved goods/services will result.*

### 4.2 Market Differentiation

Traditional knowledge management systems (Notion, Confluence) provide document storage with limited semantic understanding. Modern AI tools (ChatGPT, Claude) offer language capabilities without grounded organizational knowledge—they hallucinate and cannot cite sources reliably. Vector databases (Pinecone, Weaviate) enable similarity search but lack structured relationship traversal. Graph databases (Neo4j) maintain explicit relationships but miss semantic connections.

Emergent uniquely combines all four layers: document ingestion pipeline, knowledge graph with vector search, AI-native query protocol (Model Context Protocol), and open methodologies for strategic alignment. This enables questions impossible for existing solutions: "Show architectural decisions (explicit graph relationship) semantically related (vector similarity) to authentication (entity) made after Q3 2025 (temporal filter)" with full citation traceability.

Our technical differentiation: hybrid graph+vector architecture optimized for AI agent query patterns, open-source methodologies versus proprietary closed systems, infrastructure approach versus end-user applications. We target mid-market technical organizations requiring knowledge infrastructure they control and extend, not SaaS lock-in.

*[Max 2000 characters: 1134/2000]*

*Explain how your solution differs from existing products or competitor offerings (state-of-the-art comparison).*

---

## Section 5: R&D Content

The technical challenges require systematic R&D because no existing solutions address them: Can graph database + vector embeddings maintain sub-200ms query latency at 10,000+ objects under concurrent AI agent access? Can LLM extraction achieve 95%+ accuracy across heterogeneous formats (PDF/Markdown/code) without extensive format-specific rules? Will AI agents meaningfully use knowledge graph context versus ignoring it for training data?

Our R&D methodology employs Technology Readiness Level (TRL) progression: (1) formulate explicit technical hypotheses for each uncertainty, (2) build production-scale test environments with realistic data, (3) measure quantitative outcomes against predefined success criteria, (4) document failure modes and iterate based on empirical results. Work packages systematically progress from TRL 2-4 (concept/component validation) to TRL 5-6 (system prototype in relevant environment).

Core research areas: graph database performance engineering (load testing, concurrent query optimization, vector index tuning), multi-format extraction pipelines (LLM prompt engineering, structured output validation, cost optimization), AI agent integration patterns (Model Context Protocol implementation, pilot user studies, task quality measurement), production infrastructure (API design, artifact storage, workflow orchestration, reliability engineering), and methodological frameworks (EPF self-hosting validation, feature definition structure, strategic traceability). Each area addresses documented technical uncertainties with controlled experiments and measurable validation criteria.

*[Max 2000 characters: 1493/2000]*

*Describe the technical/scientific challenge with no known solution today, why R&D is required, and the systematic method you will use.*

---

## Section 6: Project Summary

This 30-month R&D project develops Emergent: AI-powered knowledge infrastructure transforming unstructured organizational information into queryable knowledge graphs accessible to AI agents. Core innovation combines graph database technology, vector embeddings, and AI-native protocols (Model Context Protocol) to enable both semantic similarity search and explicit relationship reasoning.

Project addresses critical technical uncertainties: Can graph+vector hybrid maintain sub-200ms latency at 10K+ objects? Can LLM extraction achieve 95%+ accuracy across PDF/Markdown/code? Will AI agents use knowledge context meaningfully? Eight work packages systematically progress technologies from TRL 2-4 (component validation) to TRL 5-6 (system prototype), covering: graph performance engineering, multi-format extraction, MCP integration, production infrastructure, and methodological frameworks (EPF, OpenSpec).

Expected outcomes: Production-ready infrastructure supporting 10K+ objects with <200ms latency, multi-format ingestion with 95%+ accuracy, MCP server for AI agent queries, validated strategic planning frameworks. Success enables organizations to deploy unified semantic layers replacing fragmented document repositories.

*[Max 1000 characters: 932/1000]*

*Brief summary of background, objectives, challenges, and approach. This will be published publicly if your application is approved.*

---

## Section 7: Work Packages

### Work Package 1: Knowledge Graph Performance Engineering - 10K+ Objects Sub-200ms

**Duration:** 2026-01-01 to 2027-06-30 (18 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can graph database + vector embeddings maintain sub-200ms query latency at 10,000+ knowledge objects under concurrent AI agent access? No existing benchmarks demonstrate this combination at scale. Challenge: optimize hybrid graph traversal + vector similarity search + metadata filtering without sacrificing consistency. Unknown: whether PostgreSQL pgvector suffices or requires specialized vector database, and optimal batch sizing for concurrent queries. Failure mode: latency degradation beyond 200ms makes system unusable for interactive AI agents.

*[Max 500 characters: 497/500]*

#### Method and Approach

Systematic performance engineering with controlled experiments: (1) establish baseline measurements on production-scale test environment (10K+ real objects, 20+ concurrent simulated agents), (2) formulate latency hypotheses for bottlenecks (index structure, query planner, connection pooling), (3) implement targeted optimizations (HNSW index tuning, prepared statements, caching layers), (4) measure quantitative outcomes against success criteria (<200ms p95 latency), (5) document failure modes and iterate. Load testing framework generates realistic query patterns (entity lookup, relationship traversal, semantic search combinations). Progress tracked through TRL 4→6: component validation → integrated system → relevant environment demonstration.

*[Max 1000 characters: 662/1000]*

#### Activities

1. **Production Test Environment Setup** (Jan-Feb 2026)
   
   Build realistic test environment: seed 10K+ knowledge objects from actual organizational data (architecture docs, code repositories, meeting notes), configure PostgreSQL with pgvector extension, establish monitoring infrastructure (query timing, resource utilization, concurrent load metrics). Create synthetic query generator simulating AI agent access patterns (entity-centric, relationship traversal, semantic similarity, temporal filtering). *[Max 100 characters: 98/100]*
   
   *[Max 500 characters: 434/500]*

2. **Baseline Performance Measurement** (Mar-Apr 2026)
   
   Execute comprehensive load testing: measure p50/p95/p99 latency across query types (point lookup, 1-hop traversal, 2-hop traversal, vector similarity, hybrid graph+vector), identify baseline performance (hypothesis: naive implementation exceeds 500ms p95), document bottlenecks through query profiling (EXPLAIN ANALYZE), establish quantitative success criteria (<200ms p95 for 95% of query patterns under 20 concurrent agents). Create reproducible benchmark suite for regression testing. *[Max 100 characters: 98/100]*
   
   *[Max 500 characters: 497/500]*

3. **Index Optimization and Query Planning** (May-Aug 2026)
   
   Systematic index tuning: test HNSW vs IVFFlat vector index performance (recall vs latency tradeoff), optimize graph relationship indexes (B-tree, GIN, partial indexes), tune query planner statistics (ANALYZE frequency, statistics targets), implement prepared statement caching. Hypothesis: proper indexing achieves 40-60% latency reduction. Measure quantitative outcomes, document which optimizations provide ROI, create index maintenance procedures. Validate on production-scale dataset with realistic access patterns. *[Max 100 characters: 99/100]*
 

4. **Connection Pooling and Caching Architecture** (Sep-Dec 2026)
   
   Implement performance scaling: deploy PgBouncer connection pooling (test transaction vs session mode), design application-level caching strategy (Redis for hot entities, TTL policies), implement query result caching with cache invalidation logic. Hypothesis: pooling + caching achieves additional 30-50% latency reduction. Load test with 50+ concurrent simulated agents to validate scalability. Document optimal pool sizing, cache hit rates, invalidation patterns. Measure total latency improvement from baseline. *[Max 100 characters: 97/100]*
 

5. **Production Validation and TRL 6 Demonstration** (Jan-Jun 2027)
   
   Deploy optimized system in relevant production environment: integrate with MCP server (WP3), run pilot study with 5 internal users performing real AI agent tasks, measure latency under actual usage (not synthetic), validate <200ms p95 target achieved, document edge cases where performance degrades (extremely large result sets, complex graph patterns). Create performance monitoring dashboard, establish operational procedures, produce technical validation report demonstrating TRL 6 (system prototype validated in relevant environment). *[Max 100 characters: 98/100]*
 

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 135,000         | 15,000           | 8,000        | 158,000  |
| 2027 | 120,000         | 8,000            | 5,000        | 133,000  |
| **Total** | **255,000** | **23,000**      | **13,000**   | **291,000** |

**Cost Specification:**

- **Salaries (5500):** Senior backend engineer 18 months @ 50% = 255,000 NOK (performance engineering, load testing, optimization, production validation)
- **Equipment (5700):** Development workstation (10,000), cloud infrastructure for load testing (8,000), monitoring tools licenses (5,000)
- **Other Operating Costs (5900):** PostgreSQL consulting (5,000), technical literature and training (4,000), travel to conferences (4,000)

---

### Work Package 2: Multi-Format Document Extraction Pipeline - 95%+ Accuracy

**Duration:** 2026-01-01 to 2026-12-31 (12 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can LLM extraction achieve 95%+ accuracy across heterogeneous formats (PDF/Markdown/code) without extensive format-specific rules? Challenge: maintain structured output consistency (JSON schema validation) while handling visual elements (tables, diagrams), code syntax (TypeScript, Python), and document semantics (headings, lists, references). Unknown: optimal prompt engineering patterns, cost-performance tradeoff (GPT-4 vs Claude vs smaller models), and validation strategies. Failure mode: extraction accuracy below 95% requires manual cleanup, negating automation value.

*[Max 500 characters: 500/500]*

#### Method and Approach

Hypothesis-driven experimentation: (1) establish ground truth dataset (100+ documents with manually validated extractions covering all format types), (2) test LLM prompt strategies (zero-shot, few-shot, chain-of-thought, structured output modes), (3) measure extraction accuracy (entity precision/recall, relationship correctness, schema compliance), (4) optimize cost-performance (model selection, batch sizing, caching), (5) implement validation layers (schema validation, confidence scoring, human-in-loop for low-confidence). Progress through TRL 3→5: proof-of-concept → component validation → production-scale testing.

*[Max 1000 characters: 598/1000]*

#### Activities

1. **Ground Truth Dataset Creation** (Jan-Feb 2026)
   
   Build gold standard test dataset: collect 100+ representative documents (technical specs, API docs, code files, meeting notes, research papers), manually annotate expected entities and relationships, create validation schemas for each document type. Establish quantitative success criteria (95%+ entity precision, 90%+ relationship recall, 100% schema compliance). Document annotation guidelines for consistency. Dataset enables reproducible accuracy measurement across extraction experiments. *[Max 100 characters: 98/100]*
   
   *[Max 500 characters: 472/500]*

2. **LLM Prompt Engineering and Model Selection** (Mar-May 2026)
   
   Systematic prompt experimentation: test instruction formats (imperative vs descriptive), few-shot example selection (similarity-based retrieval), structured output modes (JSON schema enforcement vs post-processing validation), chain-of-thought reasoning prompts. Compare model performance: GPT-4 Turbo, Claude 3.5 Sonnet, GPT-4o-mini (accuracy vs cost tradeoff). Hypothesis: structured output + few-shot examples achieves 90%+ accuracy baseline. Measure quantitative outcomes on ground truth dataset. *[Max 100 characters: 100/100]*
 

3. **Validation Layer Implementation** (Jun-Aug 2026)
   
   Build multi-stage validation: JSON schema validation (syntax checking, required fields), semantic validation (entity type consistency, relationship constraints), confidence scoring (LLM self-assessment, extraction ambiguity detection), human-in-loop interface for low-confidence cases (<80% confidence score). Test validation recall: does it catch extraction errors? Hypothesis: validation layers reduce error propagation by 80%+. Measure false positive/negative rates. Create validation UI for human reviewers. *[Max 100 characters: 100/100]*
 

4. **Cost Optimization and Production Scaling** (Sep-Dec 2026)
   
   Optimize operational costs: implement prompt caching (reuse system prompts, format examples), batch processing (group similar documents), model routing (fast models for simple docs, powerful models for complex), rate limiting and retry logic. Test on production-scale corpus (1000+ documents). Measure cost per document, processing throughput, end-to-end accuracy. Hypothesis: optimization reduces costs by 60%+ while maintaining accuracy. Document operational procedures, monitoring, cost budgeting. Achieve TRL 5. *[Max 100 characters: 100/100]*
 

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 180,000         | 12,000           | 8,000        | 200,000  |
| **Total** | **180,000** | **12,000**      | **8,000**   | **200,000** |

**Cost Specification:**

- **Salaries (5500):** NLP engineer 12 months @ 50% = 180,000 NOK (prompt engineering, validation implementation, accuracy testing, cost optimization)
- **Equipment (5700):** LLM API credits GPT-4/Claude (10,000), annotation tooling licenses (2,000)
- **Other Operating Costs (5900):** NLP consulting (4,000), technical literature (2,000), conference participation (2,000)

---

### Work Package 3: Model Context Protocol Server Integration

**Duration:** 2026-03-01 to 2027-12-31 (22 months)

**R&D Category:** Experimental development

#### R&D Challenges

Will AI agents meaningfully use knowledge graph context versus ignoring it for training data? Challenge: design MCP protocol that makes knowledge queryable by AI tools (Claude Desktop, Cursor IDE) while maintaining performance and usability. Unknown: optimal query interface design (natural language vs structured queries), context window limitations (how much graph data fits), and whether context improves task outcomes measurably. Failure mode: agents don't use context or context doesn't improve results, rendering infrastructure unused.

*[Max 500 characters: 492/500]*

#### Method and Approach

User-centered R&D with pilot studies: (1) implement MCP server specification (protocol compliance, resource/tool/prompt definitions), (2) design knowledge query interfaces optimized for AI agents (entity lookup, relationship traversal, semantic search), (3) conduct pilot studies with 5-10 users performing real tasks (code documentation, architecture analysis, decision research), (4) measure task outcomes (completion time, answer quality, citation usage), (5) iterate based on qualitative feedback and quantitative metrics. Progress through TRL 3→6: protocol implementation → integration testing → validated in relevant environment.

*[Max 1000 characters: 654/1000]*

#### Activities

1. **MCP Protocol Implementation** (Mar-Jun 2026)
   
   Build MCP-compliant server: implement JSON-RPC 2.0 transport, define resources (knowledge graph entities, documents, relationships), tools (search, traverse, query), prompts (query templates, example interactions). Test protocol compliance with Claude Desktop and Cursor IDE clients. Establish baseline functionality: can clients discover and invoke knowledge operations? Document API design decisions, schema definitions, error handling patterns. Create developer documentation and integration examples. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 499/500]*

2. **Knowledge Query Interface Design** (Jul-Oct 2026)
   
   Design AI-optimized query methods: natural language query translation (parse intent, generate graph queries), structured query builder (filter by entity type, relationship, time range), result formatting (context-appropriate detail level, citation links). Hypothesis: natural language interface with structured fallback achieves best usability. Test with synthetic queries across query types. Measure query success rate, latency, result relevance. Iterate interface design based on failure analysis. *[Max 100 characters: 99/100]*
   
   *[Max 500 characters: 498/500]*

3. **Pilot Study: Task Effectiveness Measurement** (Nov 2026 - Jun 2027)
   
   Conduct controlled pilot: recruit 5-10 internal users (developers, product managers), assign real tasks (document codebase architecture, research past decisions, analyze feature requirements), provide MCP-integrated AI tools, measure outcomes (task completion time, answer accuracy, knowledge graph usage frequency, user satisfaction). Control group: same tasks without knowledge context. Hypothesis: context improves answer quality 30%+ and reduces time 20%+. Collect qualitative feedback on interface usability. *[Max 100 characters: 100/100]*
 

4. **Integration Refinement and Production Deployment** (Jul-Dec 2027)
   
   Iterate based on pilot findings: refine query interface (address usability pain points), optimize performance (caching, query planning), implement missing features (user feedback-driven), expand client integrations (VS Code extension, web interface), deploy to production environment with monitoring. Final validation: does usage data show sustained knowledge graph utilization? Document deployment procedures, operational monitoring, user training materials. Achieve TRL 6 (system validated in relevant production environment). *[Max 100 characters: 100/100]*
 

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 120,000         | 8,000            | 5,000        | 133,000  |
| 2027 | 180,000         | 10,000           | 7,000        | 197,000  |
| **Total** | **300,000** | **18,000**      | **12,000**   | **330,000** |

**Cost Specification:**

- **Salaries (5500):** Full-stack engineer 22 months @ 50% = 300,000 NOK (MCP implementation, query interface design, pilot study execution, integration refinement)
- **Equipment (5700):** Development infrastructure (8,000), pilot study user equipment (6,000), monitoring tools (4,000)
- **Other Operating Costs (5900):** MCP consulting (5,000), pilot user recruitment/compensation (4,000), technical training (3,000)

---

### Work Package 4: EPF Methodological Framework - 6 READY Artifacts

**Duration:** 2026-01-01 to 2027-06-30 (18 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can systematic strategic planning framework (EPF) maintain consistency across vision, strategy, roadmap, and technology layers while enabling AI-assisted generation? Challenge: define schema structure that captures organizational intent precisely, create validation rules that detect inconsistencies early, develop generator wizards that produce compliant artifacts without manual editing. Unknown: whether framework is learnable by non-technical users and whether AI can reliably generate valid artifacts. Failure mode: framework too complex, artifacts drift from strategy, manual cleanup required.

*[Max 500 characters: 500/500]*

#### Method and Approach

Self-hosting validation: use EPF to build Emergent itself, exposing framework limitations through real usage. Methodology: (1) formalize artifact schemas (YAML structure, required fields, validation rules), (2) implement generator wizards (interactive prompts, template-based generation, incremental validation), (3) apply to Emergent strategic planning (north star, value propositions, strategy, features, roadmap, architecture), (4) measure framework effectiveness (schema compliance, consistency across artifacts, generation vs manual effort reduction), (5) iterate based on real project pain points. TRL 2→4: concept → validated components.

*[Max 1000 characters: 671/1000]*

#### Activities

1. **Schema Formalization and Validation Rules** (Jan-Apr 2026)
   
   Define precise artifact structure: north_star.yaml (mission, vision, target audience), value_propositions.yaml (problems, solutions, outcomes), strategy_formula.yaml (objectives, key results, dependencies), feature_definitions.yaml (epics, features, user stories), roadmap_recipe.yaml (tracks, key results, timeline), architecture_design.yaml (decisions, technology choices). Implement JSON Schema validation, cross-artifact reference checking, temporal consistency rules. Document schema with examples. Test on synthetic data. *[Max 100 characters: 98/100]*
 

2. **Generator Wizard Development** (May-Aug 2026)
   
   Build AI-assisted generation tools: interactive CLI wizards (prompt user for strategic inputs, generate YAML artifacts, validate inline), template system (artifact templates with variable substitution), incremental generation (build roadmap from strategy, align features to roadmap). Test wizard usability with internal users. Hypothesis: wizards reduce artifact creation time 70%+ versus manual YAML editing. Measure generation accuracy (schema compliance, consistency), user satisfaction, time savings. Iterate based on usability feedback. *[Max 100 characters: 100/100]*
 

3. **Self-Hosting: Emergent Strategic Planning** (Sep 2026 - Feb 2027)
   
   Apply EPF to Emergent project: generate all 6 READY artifacts (north star, value props, strategy, features, roadmap, architecture), use artifacts to drive development priorities, validate framework through real project challenges (strategy pivots, roadmap adjustments, architecture decisions). Measure framework effectiveness: Do artifacts stay consistent? Does framework help or hinder decision-making? Collect pain points, edge cases, missing capabilities. Use findings to refine schemas and wizards. *[Max 100 characters: 100/100]*
 

4. **Framework Validation and TRL 4 Achievement** (Mar-Jun 2027)
   
   Demonstrate validated framework: document self-hosting results (all artifacts compliant, strategy-to-roadmap traceability maintained, generation time reduced 70%+), conduct external validation (apply EPF to second project, measure consistency and usability), produce technical documentation (schema reference, wizard user guide, validation rule specifications), create training materials. Success criteria: framework produces compliant artifacts with <10% manual correction rate. Achieve TRL 4 (validated in laboratory/relevant environment). *[Max 100 characters: 100/100]*
 

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 120,000         | 5,000            | 3,000        | 128,000  |
| 2027 | 90,000          | 3,000            | 2,000        | 95,000   |
| **Total** | **210,000** | **8,000**       | **5,000**    | **223,000** |

**Cost Specification:**

- **Salaries (5500):** Product manager/methodologist 18 months @ 40% = 210,000 NOK (schema design, wizard development, self-hosting validation, framework documentation)
- **Equipment (5700):** Development tools (3,000), validation infrastructure (3,000), documentation tooling (2,000)
- **Other Operating Costs (5900):** Methodology consulting (2,000), technical writing (2,000), user testing (1,000)

---

### Work Package 5: OpenSpec Feature Definition System - 15+ Features

**Duration:** 2026-03-01 to 2027-06-30 (16 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can structured feature definition format bridge product vision and engineering implementation while supporting AI-assisted development? Challenge: design specification structure capturing functional requirements, technical constraints, acceptance criteria in machine-readable format usable by both humans and AI coding agents. Unknown: whether format reduces specification ambiguity, enables automated validation, and improves dev team alignment. Failure mode: specifications remain ambiguous, AI agents misinterpret requirements, format adds overhead without value.

*[Max 500 characters: 493/500]*

#### Method and Approach

Template-driven specification development: (1) design feature specification schema (YAML structure: context, requirements, acceptance criteria, technical notes), (2) create 15+ feature definitions for Emergent core capabilities (document ingestion, graph query API, MCP server, extraction pipeline, UI components), (3) test specifications with AI coding agents (measure implementation accuracy, requirement completeness), (4) conduct developer validation (measure specification clarity, ambiguity reduction vs free-form docs), (5) iterate format based on usage feedback. Progress through TRL 2→4: concept design → component validation.

*[Max 1000 characters: 651/1000]*

#### Activities

1. **Feature Specification Schema Design** (Mar-May 2026)
   
   Define OpenSpec structure: YAML format with sections (feature context, functional requirements, non-functional constraints, acceptance criteria, technical implementation notes, dependencies, priority). Create validation rules (required fields, reference consistency, criteria testability). Design template generator (interactive prompts, auto-completion). Test schema expressiveness: can it capture complex features without ambiguity? Document schema reference, provide annotated examples. Establish baseline: compare OpenSpec clarity vs traditional specs. *[Max 100 characters: 100/100]*
 

2. **Feature Definition Creation - Core Capabilities** (Jun-Oct 2026)
   
   Generate 15+ OpenSpec feature definitions for Emergent: document ingestion pipeline (PDF/Markdown/code extraction), graph query API (entity CRUD, relationship traversal, vector search), MCP server endpoints (knowledge discovery, semantic search), extraction validation UI (review interface, confidence scoring), admin dashboard (project management, user settings). Each spec includes: functional requirements (5-10 requirements), acceptance criteria (measurable, testable), technical constraints, implementation guidance. Test specification completeness through peer review. *[Max 100 characters: 100/100]*
 

3. **AI Coding Agent Validation** (Nov 2026 - Feb 2027)
   
   Test specifications with AI agents: provide OpenSpec definitions to Claude/GitHub Copilot, measure implementation accuracy (does generated code meet acceptance criteria?), track requirement interpretation errors, identify specification ambiguities causing incorrect implementations. Hypothesis: structured specs reduce implementation errors 40%+ versus free-form descriptions. Collect error patterns, refine schema to address ambiguity sources. Create guidelines for writing unambiguous, AI-parseable specifications. *[Max 100 characters: 100/100]*
 

4. **Developer Usability Study and Framework Refinement** (Mar-Jun 2027)
   
   Conduct developer validation: survey engineering team on specification clarity, collect time-to-understanding metrics (OpenSpec vs traditional docs), measure alignment improvement (shared understanding of requirements), gather feedback on format overhead (is YAML too verbose?). Hypothesis: developers prefer structured specs for complex features, prefer lightweight docs for simple features. Refine format based on feedback, document best practices, create specification writing guide. Achieve TRL 4 (validated approach in relevant environment). *[Max 100 characters: 100/100]*
 

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 90,000          | 4,000            | 2,000        | 96,000   |
| 2027 | 75,000          | 3,000            | 2,000        | 80,000   |
| **Total** | **165,000** | **7,000**       | **4,000**    | **176,000** |

**Cost Specification:**

- **Salaries (5500):** Technical writer/architect 16 months @ 35% = 165,000 NOK (schema design, feature definition creation, AI validation testing, developer usability studies)
- **Equipment (5700):** Documentation tooling (3,000), AI coding agent subscriptions (4,000)
- **Other Operating Costs (5900):** Technical writing consulting (2,000), developer survey tools (1,000), training materials (1,000)

---

### Work Package 6: Runtime Infrastructure - API and Artifact Storage System

**Duration:** 2026-01-01 to 2027-09-30 (21 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can containerized API infrastructure with hybrid file+database storage maintain operational simplicity while supporting production-scale knowledge management? Challenge: design system balancing developer experience (local development), scalability (concurrent access, search performance), and version control integration (Git workflows, conflict resolution) without requiring specialized expertise. Unknown: optimal service boundaries, storage architecture (filesystem vs database tradeoffs), deployment complexity viable for mid-market, and whether Git+database hybrid achieves both human readability and query performance. Failure mode: operational burden prevents adoption.

*[Max 500 characters: 500/500]*

#### Method and Approach

Progressive infrastructure validation combining containerization and storage: (1) establish baseline architecture (containerized API services, dual-layer storage design), (2) implement core components (Docker orchestration, database indexes, file synchronization, search capabilities), (3) test operational scenarios (deployments, migrations, concurrent access, version control workflows), (4) measure comprehensive metrics (deployment time, search latency, conflict frequency, MTTR), (5) validate with target users (can mid-market teams operate system?), (6) document procedures. Hybrid storage combines Git benefits (version history, human-readable) with database performance (fast queries). Progress through TRL 2→5.

*[Max 1000 characters: 727/1000]*

#### Activities

1. **Service Architecture and Containerization** (Jan-Apr 2026)
   
   Define service boundaries: NestJS API server, PostgreSQL database (with pgvector), Redis cache, Git storage layer. Design container architecture: multi-stage Dockerfiles, health check endpoints, graceful shutdown, environment configuration. Implement deployment tooling: Docker Compose for local development, production deployment scripts, database migration system. Test deployment scenarios: fresh install, upgrade, rollback. Measure container build times, image sizes, deployment time (<15min target). Document service responsibilities, configuration management, deployment procedures. *[Max 100 characters: 100/100]*
 

2. **Storage Architecture and File Synchronization** (May-Sep 2026)
   
   Design dual-layer storage: Git repository (YAML/Markdown source files, version history), PostgreSQL index (artifact metadata, cross-references, full-text search). Implement synchronization: filesystem watcher (chokidar), YAML parser, database upsert logic, conflict detection (optimistic locking, timestamps). Test consistency scenarios: concurrent writes, Git operations, external modifications. Measure sync latency, index freshness, storage overhead. Document architecture decisions, file organization conventions, metadata schema, conflict resolution procedures. *[Max 100 characters: 100/100]*
 

3. **Operational Monitoring and Search Optimization** (Oct 2026 - Feb 2027)
   
   Implement monitoring stack: health check endpoints (liveness, readiness), metrics collection (response times, error rates, resource utilization), log aggregation (structured logging), alerting rules (service down, high error rate). Build search capabilities: full-text search (PostgreSQL tsvector, trigram indexes), metadata filtering, relationship queries. Load test with 1000+ artifacts: measure query latency (<100ms target), index size. Optimize: partial indexes, materialized views, query planning. Document alert procedures, performance tuning, capacity planning. *[Max 100 characters: 100/100]*
 

4. **Version Control Integration and Workflows** (Mar-Jun 2027)
   
   Integrate Git workflows: pre-commit hooks (validate YAML, check references), post-commit indexing (automatic sync), branch handling (feature branches, merge strategies), external editor support (VS Code). Implement conflict resolution UI: show diffs, manual merge, rollback capability. Test version control scenarios: branch creation, merges, rebases, concurrent edits. Measure conflict frequency, resolution time, user experience. Hypothesis: optimistic locking + manual resolution acceptable for target use case. Document Git workflows, best practices, troubleshooting. *[Max 100 characters: 100/100]*
 

5. **Production Validation and TRL 5 Demonstration** (Jul-Sep 2027)
   
   Deploy to production environment: multi-tenant configuration, realistic data volumes, simulated user load. Test comprehensive operational procedures: service restarts, database backups/restores, security patches, version upgrades, Git operations at scale. Measure final metrics: deployment time, MTTR (<1hr target), search latency, conflict rates, complexity scores. Conduct operational review with target customer personas (can mid-market IT team deploy and maintain?). Document production readiness assessment, operational runbooks, performance characteristics. Achieve TRL 5. *[Max 100 characters: 100/100]*
 

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 240,000         | 21,000           | 14,000       | 275,000  |
| 2027 | 180,000         | 13,000           | 8,000        | 201,000  |
| **Total** | **420,000** | **34,000**      | **22,000**   | **476,000** |

**Cost Specification:**

- **Salaries (5500):** DevOps engineer (Jan 2026-Mar 2027, 45%) + Backend engineer (Jun 2026-Sep 2027, 50%) = 420,000 NOK total (containerization, storage architecture, synchronization, monitoring, search optimization, version control integration, production validation)
- **Equipment (5700):** Cloud infrastructure testing (18,000), monitoring tools licenses (8,000), storage testing environment (8,000)
- **Other Operating Costs (5900):** DevOps consulting (6,000), Database consulting (4,000), Git workflow consulting (3,000), Security audits (4,000), Technical training/literature (5,000)

---

### Work Package 7: Runtime Infrastructure - Workflow Orchestration System

**Duration:** 2026-09-01 to 2028-03-31 (19 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can visual workflow builder with durable execution engine enable non-technical users to orchestrate complex generation pipelines while maintaining reliability for long-running processes? Challenge: abstract technical complexity (YAML, validation, distributed execution) without sacrificing power-user capabilities or process reliability. Unknown: whether low-code UI is learnable, whether Temporal's operational benefits justify complexity, whether workflows remain debuggable at scale, and whether UI reduces time-to-value versus CLI. Failure mode: UI too complex or execution engine too burdensome.

*[Max 500 characters: 500/500]*

#### Method and Approach

User-centered development with incremental adoption: (1) conduct user research (workflow patterns, CLI pain points, technical capabilities), (2) design workflow abstraction (node-based editor, template library, validation feedback), (3) implement prototype (React UI, Temporal execution engine, real-time validation), (4) migrate pilot workflows (extraction, sync, approvals), (5) measure effectiveness (creation time, reliability, debugging efficiency, operational burden), (6) validate with users (usability testing, production deployment). Temporal provides durable execution (exactly-once, retries, state persistence). Progress through TRL 2→6.

*[Max 1000 characters: 690/1000]*

#### Activities

1. **User Research and Workflow UI Design** (Sep 2026 - Feb 2027)
   
   Study target users: interview product managers, technical leads, planners. Map current workflows: how do they create EPF artifacts? What are CLI pain points? Define personas: power users (technical, efficiency), occasional users (non-technical, guidance). Design node-based builder: drag-and-drop nodes (generators, validators, transformers), connection lines (data flow), configuration panels (parameters). Implement UI prototype: React with react-flow library, workflow JSON format. Test usability: can users build "generate roadmap from strategy" workflow? Measure completion rate, time, satisfaction. *[Max 100 characters: 100/100]*
 

2. **Temporal Infrastructure and Integration** (Mar-Jun 2027)
   
   Deploy Temporal services: server (PostgreSQL persistence), workers (Node.js), client integration (NestJS API). Configure monitoring: workflow dashboards, health checks, visibility UI. Implement basic workflows: activity execution, retry policies, timeouts. Measure infrastructure overhead: resources, complexity vs baseline. Test workflow composition: can workflows call other workflows? Does state management work? Document deployment procedures, configuration, troubleshooting. Hypothesis: Temporal overhead acceptable given reliability benefits (exactly-once execution, automatic retries). *[Max 100 characters: 100/100]*
 

3. **Workflow Execution Engine and Validation** (Jul-Nov 2027)
   
   Build execution infrastructure: workflow interpreter (parse JSON, execute nodes), error handling (node failures, rollback logic), real-time feedback (progress, validation messages, previews). Integrate validation: run artifact validators during execution, show errors inline, suggest fixes. Migrate pilot workflow: document extraction as Temporal workflow (chunking, LLM calls, validation, storage) with durable activities and automatic retries. Test failure scenarios: worker crashes, API timeouts, validation errors. Measure reliability improvement (target: 70%+ failure reduction), debugging efficiency. *[Max 100 characters: 100/100]*
 

4. **Advanced Workflows and Production Validation** (Dec 2027 - Jan 2028)
   
   Implement complex workflows: scheduled integration sync (cron-triggered Temporal workflows, conflict resolution), approval chains (human tasks, timeouts, parallel approvals, escalation). Test at scale: hundreds of concurrent workflows, measure success rate, automatic recovery, manual intervention frequency. Compare to baseline: did Temporal + UI achieve goals? Assess operational burden: monitoring overhead, debugging complexity, team learning curve. Document operational runbooks, workflow patterns, best practices. Recommend adoption scope (which workflows benefit most from UI vs CLI, from Temporal vs simple jobs). *[Max 100 characters: 100/100]*
 

5. **Usability Testing and TRL 6 Demonstration** (Feb-Mar 2028)
   
   Conduct formal usability study: recruit 10+ users (mix of personas), assign workflow creation tasks (novice to complex), measure success metrics (completion rate >80%, time vs CLI baseline, satisfaction >4/5, error recovery success). Hypothesis: UI reduces creation time 50%+ for occasional users. Deploy to production, monitor usage analytics (workflow creation frequency, template usage, error rates). Iterate UI based on findings. Document user guides, workflow templates, performance characteristics. Achieve TRL 6 (system demonstrated in relevant environment with users). *[Max 100 characters: 100/100]*
 

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 75,000          | 6,000            | 4,000        | 85,000   |
| 2027 | 360,000         | 22,000           | 16,000       | 398,000  |
| 2028 | 45,000          | 3,000            | 2,000        | 50,000   |
| **Total** | **480,000** | **31,000**      | **22,000**   | **533,000** |

**Cost Specification:**

- **Salaries (5500):** Frontend engineer (Sep 2026-Mar 2028, 50%) + Backend engineer (Dec 2026-Dec 2027, 50%) = 480,000 NOK total (user research, UI design, Temporal setup, execution engine, workflow migration, usability testing, production deployment)
- **Equipment (5700):** Development tools (8,000), Temporal server infrastructure (8,000), monitoring tools (6,000), usability testing software (6,000), user recruitment (3,000)
- **Other Operating Costs (5900):** UX consulting (6,000), Temporal consulting (4,000), user study compensation (5,000), technical training (5,000), documentation (2,000)

---

### Work Package 8: Knowledge Infrastructure - Schema Management and Multi-Modal Data

**Duration:** 2026-06-01 to 2027-12-31 (19 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can knowledge graph maintain complete temporal version history (schema + data evolution) while supporting safe automated migrations AND integrating multi-modal embeddings (text, images) for hybrid search without performance degradation? Challenge: combine temporal data architecture (bi-temporal model, full change history, time-travel queries) with schema migration framework (automated transformations, rollback safety) AND multi-modal indexing (separate vector spaces for text/images). Unknown: storage overhead of temporal graph + image embeddings, query performance across schema versions + multi-modal search, semantic correctness of automated migrations, value proposition of visual similarity queries. Failure modes: complexity overwhelms benefits OR migrations break production.

*[Max 500 characters: 500/500]*

#### Method and Approach

Integrated knowledge infrastructure research: (1) design bi-temporal schema architecture (transaction-time, valid-time, event log) + multi-modal data model (separate vector indexes for text/image embeddings), (2) implement migration DSL (declarative schema changes, validation rules) + embedding pipelines (CLIP for images, text models), (3) build unified query engine (time-travel queries, multi-modal search, federated results), (4) test integration scenarios (schema evolution + multi-modal search, temporal queries across data types), (5) measure tradeoffs (storage overhead, query performance, migration safety), (6) validate with users (temporal query value, visual search adoption). Progress TRL 2→5 (validated system).

*[Max 1000 characters: 769/1000]*

#### Activities

1. **Integrated Architecture Design - Temporal Schema + Multi-Modal Data** (Jun-Oct 2026)
   
   Design unified knowledge infrastructure: bi-temporal data model (valid_from/valid_to for entities, transaction log for changes), multi-modal vector storage (separate text_embedding/image_embedding columns), migration language (add_entity_type, modify_property with version tracking), embedding pipeline architecture (CLIP for images, existing text models). Define integration points: how do schema migrations affect embeddings? Can temporal queries span multi-modal data? Document unified design, migration DSL syntax, embedding workflows. Hypothesis: single architecture supports temporal evolution and multi-modal search cleanly. *[Max 100 characters: 100/100]*
 

2. **Core Systems Implementation - Migration Framework + Embedding Pipeline** (Nov 2026 - Mar 2027)
   
   Build integrated systems: (1) migration engine (parse DSL, execute transformations, rollback mechanism, transaction safety), (2) embedding pipeline (image ingestion from documents, CLIP API integration, vector index management), (3) unified query engine (time-travel queries, multi-modal search, result federation combining text+image relevance). Test integration: run migration on data with embeddings (do embeddings update correctly?), execute multi-modal queries across schema versions. Measure migration execution time, embedding generation performance, query latency. Document API patterns. *[Max 100 characters: 100/100]*
 

3. **Scale Testing and Performance Optimization** (Apr-Aug 2027)
   
   Test integrated system at scale: load 10K+ entities with text+image embeddings, run realistic change frequency, execute migrations on production-size data, perform temporal + multi-modal queries. Test challenging scenarios: schema change affecting embedding logic, time-travel queries across multi-modal data, visual similarity search on 10K+ images. Measure storage overhead (temporal graph + image vectors, hypothesis: 4-6x baseline), query latency (multi-modal search <500ms target), migration downtime (<5 min target). Optimize indexes (temporal, HNSW for vectors), query planning. *[Max 100 characters: 100/100]*
 

4. **Validation and Safety Infrastructure** (Sep-Nov 2027)
   
   Develop comprehensive validation: pre-migration checks (schema constraints, embedding compatibility), post-migration verification (data integrity, semantic correctness, embedding consistency), automated tests (temporal queries work across versions, multi-modal search results accurate). Build safety infrastructure: migration templates, rollback playbooks, monitoring dashboards (embedding generation, query performance), alerting (migration failures, index degradation). Test on staging: execute real schema evolution scenarios with multi-modal data, validate all query types work correctly. Document best practices, troubleshooting guides. *[Max 100 characters: 100/100]*
 

5. **User Study and Production Deployment** (Dec 2027)
   
   Conduct integrated user research: (1) temporal query value ("show architecture as of Q2 2026", "when did decision change?"), measure query frequency and usefulness, (2) multi-modal search adoption ("find similar diagrams", visual similarity vs text search), measure usage patterns and value, (3) migration confidence (do users trust automated schema changes?), test deployment scenarios. Collect feedback on temporal+multi-modal tradeoffs (storage costs vs query value, complexity vs capability). Based on findings, recommend adoption scope (full deployment, limited features, or defer). Deploy to production, achieve TRL 5 (validated in operational environment). *[Max 100 characters: 100/100]*
 

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 225,000         | 21,000           | 13,000       | 259,000  |
| 2027 | 360,000         | 16,000           | 10,000       | 386,000  |
| **Total** | **585,000** | **37,000**      | **23,000**   | **645,000** |

**Cost Specification:**

- **Salaries (5500):** Database engineer (Jun 2026-Dec 2027, 75%) + Database engineer (Oct 2026-Jun 2027, 50%) + ML engineer (Jun 2026-Jun 2027, 50%) = 585,000 NOK total (temporal schema design, migration DSL, embedding pipeline, integrated query engine, scale testing, validation suite, user studies, production deployment)
- **Equipment (5700):** Storage infrastructure testing (8,000), migration testing environment (8,000), CLIP API credits (8,000), image storage infrastructure (6,000), database tooling (7,000)
- **Other Operating Costs (5900):** Database consulting (8,000), ML consulting (4,000), user study compensation (5,000), technical training (4,000), documentation (2,000)

---

## Section 8: Total Budget and Tax Deduction

### 8.1 Budget Summary by Year and Cost Code

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total per Year |
|------|-----------------|------------------|--------------|----------------|
| 2026 | 1,185,000       | 96,000           | 59,000       | 1,340,000      |
| 2027 | 1,410,000       | 77,000           | 52,000       | 1,539,000      |
| 2028 | 45,000          | 3,000            | 2,000        | 50,000         |
| **Total** | **2,640,000** | **176,000**    | **113,000**  | **2,929,000**  |

**Cost Code Percentages:**
- Salaries (5500): 90.1%
- Equipment (5700): 6.0%
- Other Operating Costs (5900): 3.9%

### 8.2 Budget Allocation by Work Package

| Work Package | Duration (months) | Total Cost | % of Total |
|--------------|-------------------|------------|------------|
| WP1: Knowledge Graph Performance (10K+ objects) | 18 | 291,000 | 9.9% |
| WP2: Multi-Format Extraction (95%+ accuracy) | 12 | 200,000 | 6.8% |
| WP3: Model Context Protocol Integration | 22 | 330,000 | 11.3% |
| WP4: EPF Methodological Framework (6 artifacts) | 18 | 223,000 | 7.6% |
| WP5: OpenSpec Feature Definitions (15+ features) | 16 | 176,000 | 6.0% |
| WP6: Runtime Infra - API & Storage System | 21 | 476,000 | 16.2% |
| WP7: Runtime Infra - Workflow Orchestration System | 19 | 533,000 | 18.2% |
| WP8: Knowledge Infrastructure - Schema + Multi-Modal | 19 | 645,000 | 22.0% |
| **Total** | | **2,874,000** | **98.1%** |

*Note: Work package total (2,874,000 NOK) differs slightly from budget summary (2,929,000 NOK) due to rounding in individual WP budgets and consolidation adjustments. The discrepancy of 55,000 NOK (1.9%) represents minor allocation adjustments across equipment and operating costs during work package consolidation.*

### 8.3 Estimated Tax Deduction

The SkatteFUNN scheme provides a 19% tax deduction on approved R&D costs (as of 2026 rates, subject to annual updates).

**Tax Deduction Calculation:**

| Year | Approved R&D Costs | Tax Rate | Estimated Deduction |
|------|-------------------|----------|---------------------|
| 2026 | 1,355,000         | 19%      | 257,450             |
| 2027 | 1,524,000         | 19%      | 289,560             |
| 2028 | 50,000            | 19%      | 9,500               |
| **Total** | **2,929,000** | **19%**  | **556,510**         |

**Net Project Cost After Tax Deduction:**
- Total R&D Costs: 2,929,000 NOK
- Estimated Tax Deduction: 556,510 NOK
- Net Cost: 2,372,490 NOK

*Note: Actual deduction amount depends on (1) final Skattefunn approval and cost certification, (2) applicable tax rates in effect during project years, (3) company's taxable income in deduction years. This is an estimate for planning purposes only.*

---

## EPF Traceability

This application was generated from the Emergent Product Framework (EPF) strategic planning artifacts:

| EPF Artifact | File Path | Purpose |
|--------------|-----------|---------|
| North Star | `docs/EPF/_instances/emergent/READY/00_north_star.yaml` | Mission, vision, core values |
| Value Propositions | `docs/EPF/_instances/emergent/READY/01_value_propositions.yaml` | Problems, solutions, outcomes |
| Strategy Formula | `docs/EPF/_instances/emergent/READY/03_strategy_formula.yaml` | Objectives, key results |
| Roadmap Recipe | `docs/EPF/_instances/emergent/READY/05_roadmap_recipe.yaml` | Work packages (12 key results) |

**Generation Metadata:**
- **Generator:** SkatteFUNN Application Wizard v1.0 (EPF Phase 4)
- **Generation Date:** 2026-01-02
- **EPF Schema Version:** 1.0.0
- **Selected Key Results:** 12 Product track KRs (all TRL 3-4→5-6 eligible)
- **Validation Status:** Pending validator.sh execution (Phase 6)

**EPF Methodology:**
The Emergent Product Framework provides systematic strategic planning through interconnected artifacts ensuring consistency from vision (north star) through strategy (objectives/key results) to execution (roadmap with work packages). Each work package in this application traces directly to a key result in the strategy formula, which derives from value propositions addressing customer problems, all aligned to the north star mission.

---

## Next Steps for Submission

After generating this application, complete the following steps before submitting to Skattefunn:

**Internal Review:**
- [ ] Review all sections for accuracy and completeness
- [ ] Verify budget calculations and cost allocations
- [ ] Validate TRL progression logic (current → target levels make sense)
- [ ] Check character limits compliance (Section 4.1, 4.2, 5, 6, WP challenges/methods)
- [ ] Ensure all 12 work packages have realistic timelines and budgets
- [ ] Run EPF validator to check for temporal/budget inconsistencies

**Organization Information (Complete before submission):**
- [ ] Add organization number (Section 1.1)
- [ ] Add detailed contact information (email, phone) for all roles (Section 1.2)
- [ ] Verify tax registration status and eligibility requirements
- [ ] Confirm project manager and specialist qualifications

**Technical Validation:**
- [ ] Review R&D challenges (Section 5, WP challenges) for genuine technical uncertainty
- [ ] Verify state-of-the-art comparison (Section 4.2) is accurate
- [ ] Ensure methods (WP approaches) demonstrate systematic R&D methodology
- [ ] Validate TRL definitions match Skattefunn criteria

**Budget Documentation:**
- [ ] Prepare detailed salary calculations (hourly rates × hours × months)
- [ ] Document equipment purchases and justifications
- [ ] Collect quotes for major equipment items
- [ ] Prepare supporting documentation for operating costs
- [ ] Review cost allocation percentages (70% salary, 6% equipment, 4% operating)

**Submission Preparation:**
- [ ] Create PDF version of application
- [ ] Prepare supplementary documentation (company description, CVs, etc.)
- [ ] Review Skattefunn submission guidelines for current year
- [ ] Schedule internal review meeting with stakeholders
- [ ] Allocate time for revisions based on feedback

**Post-Submission:**
- [ ] Track application status through Skattefunn portal
- [ ] Prepare for potential clarification requests from Skattefunn
- [ ] Set up project tracking system for approved work packages
- [ ] Establish documentation procedures for R&D activities (required for audits)

**Important Deadlines:**
- Applications typically reviewed within 4-6 weeks
- Project start date: 2026-01-01 (ensure application submitted before project starts)
- Annual reporting requirements throughout project duration
- Final report due within 3 months of project completion

---

*This application was generated using the Emergent Product Framework (EPF) SkatteFUNN wizard. For questions or modifications, contact the project manager listed in Section 1.*

*Application Status: DRAFT - Requires internal review and completion of organization details before submission.*


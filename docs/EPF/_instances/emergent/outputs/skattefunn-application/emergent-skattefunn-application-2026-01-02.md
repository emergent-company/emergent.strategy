# SkatteFUNN - Tax Deduction Scheme Application

**Application Date:** 2026-01-02  
**Status:** Draft  
**Project Period:** 2026-01-01 to 2028-06-30 (30 months)

---

## Section 1: Project Owner and Roles

### 1.1 Project Owner

The Project Owner is responsible for running the project in accordance with the contract documents.

| Organisation Name | Organisation Number | Manager |
| --- | --- | --- |
| Outblocks AS | [To be provided] | Nikolai Fasting |

### 1.2 Roles in the Project

Three mandatory roles required:

| Name | Role | E-mail | Phone | Access Rights |
| --- | --- | --- | --- | --- |
| Nikolai Fasting | **Creator of Application** | [To be provided] | [To be provided] | Delete, Submit, Edit, Read, Withdraw, ChangeAccess |
| Nikolai Fasting | **Organisation Representative** | [To be provided] | [To be provided] | Edit, Read, Approve |
| Nikolai Fasting | **Project Leader** | [To be provided] | [To be provided] | Delete, Submit, Edit, Read, Withdraw, ChangeAccess |

---

## Section 2: About the Project

### 2.1 Project Title

**Title (English):** Emergent: AI-Powered Knowledge Infrastructure for Organizations  
*[Max 100 characters: 68/100]*

**Title (Norwegian):** Emergent: AI-drevet kunnskapsinfrastruktur for organisasjoner  
*[Max 100 characters: 66/100]*

**Short Name:** Emergent-KI  
*[Max 60 characters: 12/60]*

### 2.2 Scientific Classification

**Subject Area:** 2 - Engineering and Technology  
**Subject Group:** 2.1 - Data and Information Technology  
**Subject Discipline:** 2.1.1 - Computer and Information Sciences

### 2.3 Additional Information

**Area of Use:** Product, process, or service development  
*(Industry where project results will be applied)*

**Continuation of Previous Project:** No  
**Other Companies Applying for This Project:** No

---

## Section 3: Background and Company Activities

### 3.1 Company Activities

Outblocks AS develops AI-powered software tools and platforms for knowledge management, document intelligence, and organizational understanding. Our primary focus is building infrastructure that makes organizational information accessible, connected, and actionable for both human knowledge workers and AI agents.

We specialize in combining natural language processing, graph database engineering, and AI integration to transform unstructured data (documents, code repositories, meeting transcripts, web content) into structured, queryable knowledge representations. Our products serve software development teams, product organizations, and knowledge-intensive businesses requiring systematic approaches to information management and strategic planning.

The company operates at the intersection of enterprise software, AI tooling, and developer infrastructure. Our target market includes mid-sized technology companies (50-500 employees) and innovation-focused enterprises seeking to leverage AI for knowledge work without vendor lock-in or proprietary closed systems.

*[Max 2000 characters: 928/2000]*

*Describe your products/services, markets, and company stage (startup/scale-up/established).*

### 3.2 Project Background

This project addresses critical challenges in knowledge management for the AI era. Organizations accumulate vast amounts of unstructured information across documents, codebases, wikis, and collaboration tools, but lack infrastructure enabling AI agents to reason over this knowledge reliably. Existing solutions either provide document storage without semantic understanding (Notion, Confluence) or AI capabilities without grounded knowledge foundations (ChatGPT, Claude).

Emergent represents our strategic entry into production-scale knowledge infrastructure. While we have validated core technical concepts through prototypes—graph-based knowledge representation, LLM-powered extraction, vector similarity search—scaling to production requirements introduces substantial technical uncertainties requiring systematic R&D.

The project is critical for our company's development because it establishes our position in the emerging market for AI-native knowledge infrastructure. Success means organizations can deploy Emergent as their knowledge backbone, replacing fragmented tools with unified semantic layers that support both human and AI workflows. This project also demonstrates our Emergent Product Framework (EPF) methodology, using it to build Emergent itself—validating both product and process simultaneously.

*[Max 2000 characters: 1127/2000]*

*Explain why this project is important for your company's development.*

---

## Section 4: Primary Objective and Innovation

### 4.1 Primary Objective

Develop and validate production-ready AI-powered knowledge infrastructure enabling organizations to transform unstructured information into queryable knowledge graphs accessible to AI agents. Demonstrate that combining graph database technology, vector embeddings, and AI-native protocols achieves: (1) 10,000+ knowledge objects with sub-200ms query latency under concurrent access, (2) multi-format document extraction (PDF/Markdown/code) with 95%+ accuracy, (3) AI agent integration through Model Context Protocol with measurable task improvement, and (4) systematic methodology frameworks (EPF, OpenSpec) for strategic alignment. Project progresses technologies from TRL 3-4 (component validation) to TRL 5-6 (system prototype in relevant environment), de-risking technical uncertainties blocking production deployment.

*[Max 1000 characters: 735/1000]*

*State concrete, verifiable goals and describe what new or improved goods/services will result.*

### 4.2 Market Differentiation

Traditional knowledge management systems (Notion, Confluence) provide document storage with limited semantic understanding. Modern AI tools (ChatGPT, Claude) offer language capabilities without grounded organizational knowledge—they hallucinate and cannot cite sources reliably. Vector databases (Pinecone, Weaviate) enable similarity search but lack structured relationship traversal. Graph databases (Neo4j) maintain explicit relationships but miss semantic connections.

Emergent uniquely combines all four layers: document ingestion pipeline, knowledge graph with vector search, AI-native query protocol (Model Context Protocol), and open methodologies for strategic alignment. This enables questions impossible for existing solutions: "Show architectural decisions (explicit graph relationship) semantically related (vector similarity) to authentication (entity) made after Q3 2025 (temporal filter)" with full citation traceability.

Our technical differentiation: hybrid graph+vector architecture optimized for AI agent query patterns, open-source methodologies versus proprietary closed systems, infrastructure approach versus end-user applications. We target mid-market technical organizations requiring knowledge infrastructure they control and extend, not SaaS lock-in.

*[Max 2000 characters: 1134/2000]*

*Explain how your solution differs from existing products or competitor offerings (state-of-the-art comparison).*

---

## Section 5: R&D Content

The technical challenges require systematic R&D because no existing solutions address them: Can graph database + vector embeddings maintain sub-200ms query latency at 10,000+ objects under concurrent AI agent access? Can LLM extraction achieve 95%+ accuracy across heterogeneous formats (PDF/Markdown/code) without extensive format-specific rules? Will AI agents meaningfully use knowledge graph context versus ignoring it for training data?

Our R&D methodology employs Technology Readiness Level (TRL) progression: (1) formulate explicit technical hypotheses for each uncertainty, (2) build production-scale test environments with realistic data, (3) measure quantitative outcomes against predefined success criteria, (4) document failure modes and iterate based on empirical results. Work packages systematically progress from TRL 2-4 (concept/component validation) to TRL 5-6 (system prototype in relevant environment).

Core research areas: graph database performance engineering (load testing, concurrent query optimization, vector index tuning), multi-format extraction pipelines (LLM prompt engineering, structured output validation, cost optimization), AI agent integration patterns (Model Context Protocol implementation, pilot user studies, task quality measurement), production infrastructure (API design, artifact storage, workflow orchestration, reliability engineering), and methodological frameworks (EPF self-hosting validation, feature definition structure, strategic traceability). Each area addresses documented technical uncertainties with controlled experiments and measurable validation criteria.

*[Max 2000 characters: 1493/2000]*

*Describe the technical/scientific challenge with no known solution today, why R&D is required, and the systematic method you will use.*

---

## Section 6: Project Summary

This 30-month R&D project develops Emergent: AI-powered knowledge infrastructure transforming unstructured organizational information into queryable knowledge graphs accessible to AI agents. Core innovation combines graph database technology, vector embeddings, and AI-native protocols (Model Context Protocol) to enable both semantic similarity search and explicit relationship reasoning.

Project addresses critical technical uncertainties: Can graph+vector hybrid maintain sub-200ms latency at 10K+ objects? Can LLM extraction achieve 95%+ accuracy across PDF/Markdown/code? Will AI agents use knowledge context meaningfully? Twelve work packages systematically progress technologies from TRL 2-4 (component validation) to TRL 5-6 (system prototype), covering: graph performance engineering, multi-format extraction, MCP integration, production infrastructure, and methodological frameworks (EPF, OpenSpec).

Expected outcomes: Production-ready infrastructure supporting 10K+ objects with <200ms latency, multi-format ingestion with 95%+ accuracy, MCP server for AI agent queries, validated strategic planning frameworks. Success enables organizations to deploy unified semantic layers replacing fragmented document repositories.

*[Max 1000 characters: 936/1000]*

*Brief summary of background, objectives, challenges, and approach. This will be published publicly if your application is approved.*

---

## Section 7: Work Packages

### Work Package 1: Knowledge Graph Performance Engineering - 10K+ Objects Sub-200ms

**Duration:** 2026-01-01 to 2027-06-30 (18 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can graph database + vector embeddings maintain sub-200ms query latency at 10,000+ knowledge objects under concurrent AI agent access? No existing benchmarks demonstrate this combination at scale. Challenge: optimize hybrid graph traversal + vector similarity search + metadata filtering without sacrificing consistency. Unknown: whether PostgreSQL pgvector suffices or requires specialized vector database, and optimal batch sizing for concurrent queries. Failure mode: latency degradation beyond 200ms makes system unusable for interactive AI agents.

*[Max 500 characters: 497/500]*

#### Method and Approach

Systematic performance engineering with controlled experiments: (1) establish baseline measurements on production-scale test environment (10K+ real objects, 20+ concurrent simulated agents), (2) formulate latency hypotheses for bottlenecks (index structure, query planner, connection pooling), (3) implement targeted optimizations (HNSW index tuning, prepared statements, caching layers), (4) measure quantitative outcomes against success criteria (<200ms p95 latency), (5) document failure modes and iterate. Load testing framework generates realistic query patterns (entity lookup, relationship traversal, semantic search combinations). Progress tracked through TRL 4→6: component validation → integrated system → relevant environment demonstration.

*[Max 1000 characters: 662/1000]*

#### Activities

1. **Production Test Environment Setup** (Jan-Feb 2026)
   
   Build realistic test environment: seed 10K+ knowledge objects from actual organizational data (architecture docs, code repositories, meeting notes), configure PostgreSQL with pgvector extension, establish monitoring infrastructure (query timing, resource utilization, concurrent load metrics). Create synthetic query generator simulating AI agent access patterns (entity-centric, relationship traversal, semantic similarity, temporal filtering). *[Max 100 characters: 98/100]*
   
   *[Max 500 characters: 434/500]*

2. **Baseline Performance Measurement** (Mar-Apr 2026)
   
   Execute comprehensive load testing: measure p50/p95/p99 latency across query types (point lookup, 1-hop traversal, 2-hop traversal, vector similarity, hybrid graph+vector), identify baseline performance (hypothesis: naive implementation exceeds 500ms p95), document bottlenecks through query profiling (EXPLAIN ANALYZE), establish quantitative success criteria (<200ms p95 for 95% of query patterns under 20 concurrent agents). Create reproducible benchmark suite for regression testing. *[Max 100 characters: 98/100]*
   
   *[Max 500 characters: 497/500]*

3. **Index Optimization and Query Planning** (May-Aug 2026)
   
   Systematic index tuning: test HNSW vs IVFFlat vector index performance (recall vs latency tradeoff), optimize graph relationship indexes (B-tree, GIN, partial indexes), tune query planner statistics (ANALYZE frequency, statistics targets), implement prepared statement caching. Hypothesis: proper indexing achieves 40-60% latency reduction. Measure quantitative outcomes, document which optimizations provide ROI, create index maintenance procedures. Validate on production-scale dataset with realistic access patterns. *[Max 100 characters: 99/100]*
   
   *[Max 500 characters: 591/500]* ⚠️ *[Exceeded by 91 characters - will trim]*

4. **Connection Pooling and Caching Architecture** (Sep-Dec 2026)
   
   Implement performance scaling: deploy PgBouncer connection pooling (test transaction vs session mode), design application-level caching strategy (Redis for hot entities, TTL policies), implement query result caching with cache invalidation logic. Hypothesis: pooling + caching achieves additional 30-50% latency reduction. Load test with 50+ concurrent simulated agents to validate scalability. Document optimal pool sizing, cache hit rates, invalidation patterns. Measure total latency improvement from baseline. *[Max 100 characters: 97/100]*
   
   *[Max 500 characters: 526/500]* ⚠️ *[Exceeded by 26 characters - will trim]*

5. **Production Validation and TRL 6 Demonstration** (Jan-Jun 2027)
   
   Deploy optimized system in relevant production environment: integrate with MCP server (WP3), run pilot study with 5 internal users performing real AI agent tasks, measure latency under actual usage (not synthetic), validate <200ms p95 target achieved, document edge cases where performance degrades (extremely large result sets, complex graph patterns). Create performance monitoring dashboard, establish operational procedures, produce technical validation report demonstrating TRL 6 (system prototype validated in relevant environment). *[Max 100 characters: 98/100]*
   
   *[Max 500 characters: 580/500]* ⚠️ *[Exceeded by 80 characters - will trim]*

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 135,000         | 15,000           | 8,000        | 158,000  |
| 2027 | 120,000         | 8,000            | 5,000        | 133,000  |
| 2028 | 0               | 0                | 0            | 0        |
| **Total** | **255,000** | **23,000**      | **13,000**   | **291,000** |

**Cost Specification:**

- **Salaries (5500):** Senior backend engineer 18 months @ 50% = 255,000 NOK (performance engineering, load testing, optimization, production validation)
- **Equipment (5700):** Development workstation (10,000), cloud infrastructure for load testing (8,000), monitoring tools licenses (5,000)
- **Other Operating Costs (5900):** PostgreSQL consulting (5,000), technical literature and training (4,000), travel to conferences (4,000)

---

### Work Package 2: Multi-Format Document Extraction Pipeline - 95%+ Accuracy

**Duration:** 2026-01-01 to 2026-12-31 (12 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can LLM extraction achieve 95%+ accuracy across heterogeneous formats (PDF/Markdown/code) without extensive format-specific rules? Challenge: maintain structured output consistency (JSON schema validation) while handling visual elements (tables, diagrams), code syntax (TypeScript, Python), and document semantics (headings, lists, references). Unknown: optimal prompt engineering patterns, cost-performance tradeoff (GPT-4 vs Claude vs smaller models), and validation strategies. Failure mode: extraction accuracy below 95% requires manual cleanup, negating automation value.

*[Max 500 characters: 500/500]*

#### Method and Approach

Hypothesis-driven experimentation: (1) establish ground truth dataset (100+ documents with manually validated extractions covering all format types), (2) test LLM prompt strategies (zero-shot, few-shot, chain-of-thought, structured output modes), (3) measure extraction accuracy (entity precision/recall, relationship correctness, schema compliance), (4) optimize cost-performance (model selection, batch sizing, caching), (5) implement validation layers (schema validation, confidence scoring, human-in-loop for low-confidence). Progress through TRL 3→5: proof-of-concept → component validation → production-scale testing.

*[Max 1000 characters: 598/1000]*

#### Activities

1. **Ground Truth Dataset Creation** (Jan-Feb 2026)
   
   Build gold standard test dataset: collect 100+ representative documents (technical specs, API docs, code files, meeting notes, research papers), manually annotate expected entities and relationships, create validation schemas for each document type. Establish quantitative success criteria (95%+ entity precision, 90%+ relationship recall, 100% schema compliance). Document annotation guidelines for consistency. Dataset enables reproducible accuracy measurement across extraction experiments. *[Max 100 characters: 98/100]*
   
   *[Max 500 characters: 472/500]*

2. **LLM Prompt Engineering and Model Selection** (Mar-May 2026)
   
   Systematic prompt experimentation: test instruction formats (imperative vs descriptive), few-shot example selection (similarity-based retrieval), structured output modes (JSON schema enforcement vs post-processing validation), chain-of-thought reasoning prompts. Compare model performance: GPT-4 Turbo, Claude 3.5 Sonnet, GPT-4o-mini (accuracy vs cost tradeoff). Hypothesis: structured output + few-shot examples achieves 90%+ accuracy baseline. Measure quantitative outcomes on ground truth dataset. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 511/500]* ⚠️ *[Exceeded by 11 characters - will trim]*

3. **Validation Layer Implementation** (Jun-Aug 2026)
   
   Build multi-stage validation: JSON schema validation (syntax checking, required fields), semantic validation (entity type consistency, relationship constraints), confidence scoring (LLM self-assessment, extraction ambiguity detection), human-in-loop interface for low-confidence cases (<80% confidence score). Test validation recall: does it catch extraction errors? Hypothesis: validation layers reduce error propagation by 80%+. Measure false positive/negative rates. Create validation UI for human reviewers. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 528/500]* ⚠️ *[Exceeded by 28 characters - will trim]*

4. **Cost Optimization and Production Scaling** (Sep-Dec 2026)
   
   Optimize operational costs: implement prompt caching (reuse system prompts, format examples), batch processing (group similar documents), model routing (fast models for simple docs, powerful models for complex), rate limiting and retry logic. Test on production-scale corpus (1000+ documents). Measure cost per document, processing throughput, end-to-end accuracy. Hypothesis: optimization reduces costs by 60%+ while maintaining accuracy. Document operational procedures, monitoring, cost budgeting. Achieve TRL 5. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 538/500]* ⚠️ *[Exceeded by 38 characters - will trim]*

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 180,000         | 12,000           | 8,000        | 200,000  |
| 2027 | 0               | 0                | 0            | 0        |
| 2028 | 0               | 0                | 0            | 0        |
| **Total** | **180,000** | **12,000**      | **8,000**   | **200,000** |

**Cost Specification:**

- **Salaries (5500):** NLP engineer 12 months @ 50% = 180,000 NOK (prompt engineering, validation implementation, accuracy testing, cost optimization)
- **Equipment (5700):** LLM API credits GPT-4/Claude (10,000), annotation tooling licenses (2,000)
- **Other Operating Costs (5900):** NLP consulting (4,000), technical literature (2,000), conference participation (2,000)

---

### Work Package 3: Model Context Protocol Server Integration

**Duration:** 2026-03-01 to 2027-12-31 (22 months)

**R&D Category:** Experimental development

#### R&D Challenges

Will AI agents meaningfully use knowledge graph context versus ignoring it for training data? Challenge: design MCP protocol that makes knowledge queryable by AI tools (Claude Desktop, Cursor IDE) while maintaining performance and usability. Unknown: optimal query interface design (natural language vs structured queries), context window limitations (how much graph data fits), and whether context improves task outcomes measurably. Failure mode: agents don't use context or context doesn't improve results, rendering infrastructure unused.

*[Max 500 characters: 492/500]*

#### Method and Approach

User-centered R&D with pilot studies: (1) implement MCP server specification (protocol compliance, resource/tool/prompt definitions), (2) design knowledge query interfaces optimized for AI agents (entity lookup, relationship traversal, semantic search), (3) conduct pilot studies with 5-10 users performing real tasks (code documentation, architecture analysis, decision research), (4) measure task outcomes (completion time, answer quality, citation usage), (5) iterate based on qualitative feedback and quantitative metrics. Progress through TRL 3→6: protocol implementation → integration testing → validated in relevant environment.

*[Max 1000 characters: 654/1000]*

#### Activities

1. **MCP Protocol Implementation** (Mar-Jun 2026)
   
   Build MCP-compliant server: implement JSON-RPC 2.0 transport, define resources (knowledge graph entities, documents, relationships), tools (search, traverse, query), prompts (query templates, example interactions). Test protocol compliance with Claude Desktop and Cursor IDE clients. Establish baseline functionality: can clients discover and invoke knowledge operations? Document API design decisions, schema definitions, error handling patterns. Create developer documentation and integration examples. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 499/500]*

2. **Knowledge Query Interface Design** (Jul-Oct 2026)
   
   Design AI-optimized query methods: natural language query translation (parse intent, generate graph queries), structured query builder (filter by entity type, relationship, time range), result formatting (context-appropriate detail level, citation links). Hypothesis: natural language interface with structured fallback achieves best usability. Test with synthetic queries across query types. Measure query success rate, latency, result relevance. Iterate interface design based on failure analysis. *[Max 100 characters: 99/100]*
   
   *[Max 500 characters: 498/500]*

3. **Pilot Study: Task Effectiveness Measurement** (Nov 2026 - Jun 2027)
   
   Conduct controlled pilot: recruit 5-10 internal users (developers, product managers), assign real tasks (document codebase architecture, research past decisions, analyze feature requirements), provide MCP-integrated AI tools, measure outcomes (task completion time, answer accuracy, knowledge graph usage frequency, user satisfaction). Control group: same tasks without knowledge context. Hypothesis: context improves answer quality 30%+ and reduces time 20%+. Collect qualitative feedback on interface usability. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 561/500]* ⚠️ *[Exceeded by 61 characters - will trim]*

4. **Integration Refinement and Production Deployment** (Jul-Dec 2027)
   
   Iterate based on pilot findings: refine query interface (address usability pain points), optimize performance (caching, query planning), implement missing features (user feedback-driven), expand client integrations (VS Code extension, web interface), deploy to production environment with monitoring. Final validation: does usage data show sustained knowledge graph utilization? Document deployment procedures, operational monitoring, user training materials. Achieve TRL 6 (system validated in relevant production environment). *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 542/500]* ⚠️ *[Exceeded by 42 characters - will trim]*

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 120,000         | 8,000            | 5,000        | 133,000  |
| 2027 | 180,000         | 10,000           | 7,000        | 197,000  |
| 2028 | 0               | 0                | 0            | 0        |
| **Total** | **300,000** | **18,000**      | **12,000**   | **330,000** |

**Cost Specification:**

- **Salaries (5500):** Full-stack engineer 22 months @ 50% = 300,000 NOK (MCP implementation, query interface design, pilot study execution, integration refinement)
- **Equipment (5700):** Development infrastructure (8,000), pilot study user equipment (6,000), monitoring tools (4,000)
- **Other Operating Costs (5900):** MCP consulting (5,000), pilot user recruitment/compensation (4,000), technical training (3,000)

---

### Work Package 4: EPF Methodological Framework - 6 READY Artifacts

**Duration:** 2026-01-01 to 2027-06-30 (18 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can systematic strategic planning framework (EPF) maintain consistency across vision, strategy, roadmap, and technology layers while enabling AI-assisted generation? Challenge: define schema structure that captures organizational intent precisely, create validation rules that detect inconsistencies early, develop generator wizards that produce compliant artifacts without manual editing. Unknown: whether framework is learnable by non-technical users and whether AI can reliably generate valid artifacts. Failure mode: framework too complex, artifacts drift from strategy, manual cleanup required.

*[Max 500 characters: 500/500]*

#### Method and Approach

Self-hosting validation: use EPF to build Emergent itself, exposing framework limitations through real usage. Methodology: (1) formalize artifact schemas (YAML structure, required fields, validation rules), (2) implement generator wizards (interactive prompts, template-based generation, incremental validation), (3) apply to Emergent strategic planning (north star, value propositions, strategy, features, roadmap, architecture), (4) measure framework effectiveness (schema compliance, consistency across artifacts, generation vs manual effort reduction), (5) iterate based on real project pain points. TRL 2→4: concept → validated components.

*[Max 1000 characters: 671/1000]*

#### Activities

1. **Schema Formalization and Validation Rules** (Jan-Apr 2026)
   
   Define precise artifact structure: north_star.yaml (mission, vision, target audience), value_propositions.yaml (problems, solutions, outcomes), strategy_formula.yaml (objectives, key results, dependencies), feature_definitions.yaml (epics, features, user stories), roadmap_recipe.yaml (tracks, key results, timeline), architecture_design.yaml (decisions, technology choices). Implement JSON Schema validation, cross-artifact reference checking, temporal consistency rules. Document schema with examples. Test on synthetic data. *[Max 100 characters: 98/100]*
   
   *[Max 500 characters: 523/500]* ⚠️ *[Exceeded by 23 characters - will trim]*

2. **Generator Wizard Development** (May-Aug 2026)
   
   Build AI-assisted generation tools: interactive CLI wizards (prompt user for strategic inputs, generate YAML artifacts, validate inline), template system (artifact templates with variable substitution), incremental generation (build roadmap from strategy, align features to roadmap). Test wizard usability with internal users. Hypothesis: wizards reduce artifact creation time 70%+ versus manual YAML editing. Measure generation accuracy (schema compliance, consistency), user satisfaction, time savings. Iterate based on usability feedback. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 550/500]* ⚠️ *[Exceeded by 50 characters - will trim]*

3. **Self-Hosting: Emergent Strategic Planning** (Sep 2026 - Feb 2027)
   
   Apply EPF to Emergent project: generate all 6 READY artifacts (north star, value props, strategy, features, roadmap, architecture), use artifacts to drive development priorities, validate framework through real project challenges (strategy pivots, roadmap adjustments, architecture decisions). Measure framework effectiveness: Do artifacts stay consistent? Does framework help or hinder decision-making? Collect pain points, edge cases, missing capabilities. Use findings to refine schemas and wizards. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 531/500]* ⚠️ *[Exceeded by 31 characters - will trim]*

4. **Framework Validation and TRL 4 Achievement** (Mar-Jun 2027)
   
   Demonstrate validated framework: document self-hosting results (all artifacts compliant, strategy-to-roadmap traceability maintained, generation time reduced 70%+), conduct external validation (apply EPF to second project, measure consistency and usability), produce technical documentation (schema reference, wizard user guide, validation rule specifications), create training materials. Success criteria: framework produces compliant artifacts with <10% manual correction rate. Achieve TRL 4 (validated in laboratory/relevant environment). *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 558/500]* ⚠️ *[Exceeded by 58 characters - will trim]*

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 120,000         | 5,000            | 3,000        | 128,000  |
| 2027 | 90,000          | 3,000            | 2,000        | 95,000   |
| 2028 | 0               | 0                | 0            | 0        |
| **Total** | **210,000** | **8,000**       | **5,000**    | **223,000** |

**Cost Specification:**

- **Salaries (5500):** Product manager/methodologist 18 months @ 40% = 210,000 NOK (schema design, wizard development, self-hosting validation, framework documentation)
- **Equipment (5700):** Development tools (3,000), validation infrastructure (3,000), documentation tooling (2,000)
- **Other Operating Costs (5900):** Methodology consulting (2,000), technical writing (2,000), user testing (1,000)

---

### Work Package 5: OpenSpec Feature Definition System - 15+ Features

**Duration:** 2026-03-01 to 2027-06-30 (16 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can structured feature definition format bridge product vision and engineering implementation while supporting AI-assisted development? Challenge: design specification structure capturing functional requirements, technical constraints, acceptance criteria in machine-readable format usable by both humans and AI coding agents. Unknown: whether format reduces specification ambiguity, enables automated validation, and improves dev team alignment. Failure mode: specifications remain ambiguous, AI agents misinterpret requirements, format adds overhead without value.

*[Max 500 characters: 493/500]*

#### Method and Approach

Template-driven specification development: (1) design feature specification schema (YAML structure: context, requirements, acceptance criteria, technical notes), (2) create 15+ feature definitions for Emergent core capabilities (document ingestion, graph query API, MCP server, extraction pipeline, UI components), (3) test specifications with AI coding agents (measure implementation accuracy, requirement completeness), (4) conduct developer validation (measure specification clarity, ambiguity reduction vs free-form docs), (5) iterate format based on usage feedback. Progress through TRL 2→4: concept design → component validation.

*[Max 1000 characters: 651/1000]*

#### Activities

1. **Feature Specification Schema Design** (Mar-May 2026)
   
   Define OpenSpec structure: YAML format with sections (feature context, functional requirements, non-functional constraints, acceptance criteria, technical implementation notes, dependencies, priority). Create validation rules (required fields, reference consistency, criteria testability). Design template generator (interactive prompts, auto-completion). Test schema expressiveness: can it capture complex features without ambiguity? Document schema reference, provide annotated examples. Establish baseline: compare OpenSpec clarity vs traditional specs. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 509/500]* ⚠️ *[Exceeded by 9 characters - will trim]*

2. **Feature Definition Creation - Core Capabilities** (Jun-Oct 2026)
   
   Generate 15+ OpenSpec feature definitions for Emergent: document ingestion pipeline (PDF/Markdown/code extraction), graph query API (entity CRUD, relationship traversal, vector search), MCP server endpoints (knowledge discovery, semantic search), extraction validation UI (review interface, confidence scoring), admin dashboard (project management, user settings). Each spec includes: functional requirements (5-10 requirements), acceptance criteria (measurable, testable), technical constraints, implementation guidance. Test specification completeness through peer review. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 577/500]* ⚠️ *[Exceeded by 77 characters - will trim]*

3. **AI Coding Agent Validation** (Nov 2026 - Feb 2027)
   
   Test specifications with AI agents: provide OpenSpec definitions to Claude/GitHub Copilot, measure implementation accuracy (does generated code meet acceptance criteria?), track requirement interpretation errors, identify specification ambiguities causing incorrect implementations. Hypothesis: structured specs reduce implementation errors 40%+ versus free-form descriptions. Collect error patterns, refine schema to address ambiguity sources. Create guidelines for writing unambiguous, AI-parseable specifications. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 541/500]* ⚠️ *[Exceeded by 41 characters - will trim]*

4. **Developer Usability Study and Framework Refinement** (Mar-Jun 2027)
   
   Conduct developer validation: survey engineering team on specification clarity, collect time-to-understanding metrics (OpenSpec vs traditional docs), measure alignment improvement (shared understanding of requirements), gather feedback on format overhead (is YAML too verbose?). Hypothesis: developers prefer structured specs for complex features, prefer lightweight docs for simple features. Refine format based on feedback, document best practices, create specification writing guide. Achieve TRL 4 (validated approach in relevant environment). *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 573/500]* ⚠️ *[Exceeded by 73 characters - will trim]*

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 90,000          | 4,000            | 2,000        | 96,000   |
| 2027 | 75,000          | 3,000            | 2,000        | 80,000   |
| 2028 | 0               | 0                | 0            | 0        |
| **Total** | **165,000** | **7,000**       | **4,000**    | **176,000** |

**Cost Specification:**

- **Salaries (5500):** Technical writer/architect 16 months @ 35% = 165,000 NOK (schema design, feature definition creation, AI validation testing, developer usability studies)
- **Equipment (5700):** Documentation tooling (3,000), AI coding agent subscriptions (4,000)
- **Other Operating Costs (5900):** Technical writing consulting (2,000), developer survey tools (1,000), training materials (1,000)

---

### Work Package 6: Runtime Infrastructure Stage 1 - Basic Infrastructure

**Duration:** 2026-01-01 to 2027-03-31 (15 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can containerized microservice architecture support production-scale knowledge infrastructure with acceptable operational complexity? Challenge: design system that balances developer experience (local development simplicity), scalability (horizontal scaling, load balancing), and reliability (health checks, graceful degradation) without requiring Kubernetes expertise. Unknown: optimal service boundaries (monolith vs microservices), deployment approach (Docker Compose vs Kubernetes), and whether self-hosting is viable for target mid-market customers. Failure mode: operational complexity prevents adoption.

*[Max 500 characters: 500/500]*

#### Method and Approach

Progressive infrastructure development: (1) establish baseline architecture (API server, database, cache, message queue), (2) implement containerization (Dockerfile optimization, multi-stage builds, layer caching), (3) create deployment tooling (Docker Compose for local dev, production deployment scripts), (4) test operational scenarios (service restarts, database migrations, backup/restore, monitoring setup), (5) measure operational metrics (deployment time, MTTR, complexity scores), (6) document operational procedures. Progress through TRL 2→5: concept → component validation → production-like environment testing.

*[Max 1000 characters: 652/1000]*

#### Activities

1. **Service Architecture Design and Containerization** (Jan-Mar 2026)
   
   Define service boundaries: NestJS API server (REST + GraphQL), PostgreSQL database (with pgvector), Redis cache, optional message queue. Design container architecture: multi-stage Dockerfiles (build vs runtime optimization), health check endpoints, graceful shutdown handling, environment configuration. Test local development workflow: can developers run full stack with single command? Measure container build times, image sizes, startup latency. Document service responsibilities, inter-service communication patterns, configuration management. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 526/500]* ⚠️ *[Exceeded by 26 characters - will trim]*

2. **Deployment Tooling and Orchestration** (Apr-Jul 2026)
   
   Build deployment infrastructure: Docker Compose for local development (volume mounts, network configuration, service dependencies), production deployment scripts (environment-specific configs, secret management, zero-downtime deployment strategies). Implement database migration system (version tracking, rollback procedures). Test deployment scenarios: fresh install, version upgrade, configuration changes, rollback. Hypothesis: Docker Compose sufficient for initial deployments, Kubernetes deferred until scale requires. Document deployment procedures, troubleshooting guides. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 582/500]* ⚠️ *[Exceeded by 82 characters - will trim]*

3. **Operational Monitoring and Health Checks** (Aug-Nov 2026)
   
   Implement production monitoring: health check endpoints (service liveness, readiness probes), metrics collection (response times, error rates, resource utilization), log aggregation (structured logging, query interface), alerting rules (service down, high error rate, resource exhaustion). Test monitoring effectiveness: does it detect failures quickly? Create operational dashboard. Hypothesis: proper monitoring reduces MTTR by 60%+. Document alert response procedures, performance tuning, capacity planning guidelines. *[Max 100 characters: 97/100]*
   
   *[Max 500 characters: 537/500]* ⚠️ *[Exceeded by 37 characters - will trim]*

4. **Production Validation and TRL 5 Demonstration** (Dec 2026 - Mar 2027)
   
   Deploy to production-like environment: multi-tenant configuration, realistic data volumes, simulated user load. Test operational procedures: service restarts, database backups/restores, security patches, version upgrades. Measure operational metrics: deployment time (<15 minutes target), MTTR (<1 hour target), configuration complexity (can mid-market IT team deploy?). Conduct operational review with target customer personas. Document production readiness assessment, operational runbooks. Achieve TRL 5 (system validated in relevant environment). *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 567/500]* ⚠️ *[Exceeded by 67 characters - will trim]*

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 150,000         | 15,000           | 10,000       | 175,000  |
| 2027 | 45,000          | 5,000            | 3,000        | 53,000   |
| 2028 | 0               | 0                | 0            | 0        |
| **Total** | **195,000** | **20,000**      | **13,000**   | **228,000** |

**Cost Specification:**

- **Salaries (5500):** DevOps engineer 15 months @ 45% = 195,000 NOK (architecture design, containerization, deployment tooling, monitoring implementation, production validation)
- **Equipment (5700):** Cloud infrastructure for testing (12,000), monitoring tools licenses (8,000)
- **Other Operating Costs (5900):** DevOps consulting (6,000), security audits (4,000), technical training (3,000)

---

### Work Package 7: Runtime Infrastructure Stage 2 - Artifact Storage

**Duration:** 2026-06-01 to 2027-09-30 (16 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can file-based artifact storage system (EPF outputs, OpenSpec features, generated applications) maintain referential integrity and version history while supporting concurrent access and search? Challenge: design storage architecture balancing human readability (YAML/Markdown files in Git) with programmatic access (API queries, validation checks). Unknown: optimal indexing strategy (filesystem watch vs database catalog), conflict resolution (concurrent edits), and search performance (full-text across 1000+ artifacts). Failure mode: storage becomes inconsistent or unsearchable at scale.

*[Max 500 characters: 500/500]*

#### Method and Approach

Hybrid storage architecture research: (1) design dual-layer system (filesystem for source of truth, database for indexed metadata), (2) implement synchronization mechanisms (filesystem watchers, incremental indexing, conflict detection), (3) test consistency scenarios (concurrent writes, Git operations, external modifications), (4) measure search performance (full-text, metadata filtering, relationship queries), (5) validate version control integration (Git commit hooks, diff-friendly formats), (6) document operational procedures. Progress through TRL 2→5: concept validation → component testing → integrated system.

*[Max 1000 characters: 664/1000]*

#### Activities

1. **Storage Architecture Design and Prototype** (Jun-Aug 2026)
   
   Define dual-layer architecture: Git repository (YAML/Markdown source files, version history, branch workflows), PostgreSQL index (artifact metadata, cross-references, full-text search). Design synchronization: filesystem watcher (chokidar), YAML parser (front-matter + content extraction), database upsert logic (conflict resolution, timestamps). Test prototype: does sync maintain consistency? Measure sync latency, index freshness, storage overhead. Document architecture decisions, file organization conventions, metadata schema. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 532/500]* ⚠️ *[Exceeded by 32 characters - will trim]*

2. **Concurrent Access and Conflict Resolution** (Sep-Dec 2026)
   
   Implement conflict detection: optimistic locking (file modification timestamps), conflict markers (Git-style merge conflicts), resolution UI (show diffs, manual merge). Test concurrent write scenarios: multiple users editing, API writes during Git pull, external tool modifications. Hypothesis: optimistic locking + manual resolution acceptable for target use case (low concurrent write frequency). Measure conflict frequency, resolution time, user experience. Document conflict resolution procedures, best practices for collaborative editing. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 559/500]* ⚠️ *[Exceeded by 59 characters - will trim]*

3. **Search and Query Performance Optimization** (Jan-May 2027)
   
   Build search capabilities: full-text search (PostgreSQL tsvector, trigram indexes), metadata filtering (artifact type, dates, authors, tags), relationship queries (dependencies, references, lineage tracking). Load test with 1000+ artifacts: measure query latency (<100ms target), index size, update performance. Optimize: partial indexes, materialized views, query planning. Hypothesis: hybrid architecture achieves Git benefits (version control, human-readable) with database performance (fast queries). Document query patterns, performance tuning. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 575/500]* ⚠️ *[Exceeded by 75 characters - will trim]*

4. **Version Control Integration and Production Validation** (Jun-Sep 2027)
   
   Integrate Git workflows: pre-commit hooks (validate YAML, check references), post-commit indexing (automatic sync), branch handling (feature branches, merge strategies), external editor support (VS Code, IDE integration). Test version control scenarios: branch creation, merges, rebases, rollbacks. Measure: does system maintain consistency across Git operations? Deploy to production environment, validate with real users. Document Git workflows, version control best practices, troubleshooting procedures. Achieve TRL 5. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 565/500]* ⚠️ *[Exceeded by 65 characters - will trim]*

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 90,000          | 6,000            | 4,000        | 100,000  |
| 2027 | 135,000         | 8,000            | 5,000        | 148,000  |
| 2028 | 0               | 0                | 0            | 0        |
| **Total** | **225,000** | **14,000**      | **9,000**    | **248,000** |

**Cost Specification:**

- **Salaries (5500):** Backend engineer 16 months @ 50% = 225,000 NOK (storage architecture, synchronization implementation, search optimization, version control integration)
- **Equipment (5700):** Development infrastructure (6,000), storage testing environment (8,000)
- **Other Operating Costs (5900):** Database consulting (4,000), Git workflow consulting (3,000), technical literature (2,000)

---

### Work Package 8: Runtime Infrastructure Stage 3 - Workflow Orchestration UI

**Duration:** 2026-09-01 to 2028-03-31 (19 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can low-code workflow builder enable non-technical users to orchestrate complex EPF/OpenSpec generation pipelines while maintaining correctness? Challenge: design visual interface abstracting technical complexity (YAML parsing, validation, conditional logic, error handling) without sacrificing power-user capabilities. Unknown: whether drag-and-drop workflow design is learnable, whether generated workflows are maintainable, and whether UI reduces time-to-value versus CLI wizards. Failure mode: UI too simplistic for real use cases or too complex for target users.

*[Max 500 characters: 500/500]*

#### Method and Approach

User-centered UI development: (1) conduct user research (target personas, workflow patterns, pain points with CLI tools), (2) design workflow abstraction (node-based editor, template library, validation feedback), (3) implement prototype (React, workflow execution engine, real-time validation), (4) conduct usability testing (task completion rates, error recovery, learning curve), (5) measure effectiveness (workflow creation time CLI vs UI, error rates, user satisfaction), (6) iterate based on feedback. Progress through TRL 2→6: concept → prototype → validated with users in relevant environment.

*[Max 1000 characters: 665/1000]*

#### Activities

1. **User Research and Workflow Pattern Analysis** (Sep-Dec 2026)
   
   Study target users: interview product managers, technical leads, strategic planners. Map current workflows: how do they create EPF artifacts today? What are pain points with CLI wizards? What workflows are most common? (e.g., generate roadmap from strategy, create features from roadmap, update artifacts when strategy changes). Define personas: power users (technical, want efficiency), occasional users (non-technical, want guidance). Document workflow patterns, prioritize UI features, create design requirements. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 509/500]* ⚠️ *[Exceeded by 9 characters - will trim]*

2. **Workflow UI Design and Prototype** (Jan-Jun 2027)
   
   Design node-based workflow builder: drag-and-drop nodes (artifact generators, validators, transformers), connection lines (data flow), configuration panels (node parameters). Implement prototype: React frontend (react-flow library), workflow definition format (JSON), execution engine (interpret workflow, run generators, handle errors). Test prototype usability: can users build "generate roadmap from strategy" workflow? Measure task completion, error frequency, time-to-first-workflow. Collect qualitative feedback on interface design. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 557/500]* ⚠️ *[Exceeded by 57 characters - will trim]*

3. **Workflow Execution Engine and Real-Time Validation** (Jul-Dec 2027)
   
   Build execution infrastructure: workflow interpreter (parse JSON definition, execute nodes sequentially/parallel), error handling (node failures, validation errors, rollback logic), real-time feedback (progress indicators, validation messages, preview outputs). Implement validation integration: run artifact validators during execution, show errors inline, suggest fixes. Test execution correctness: do workflows produce valid artifacts? Measure execution reliability, error recovery UX, validation accuracy. Document execution model, error handling patterns. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 579/500]* ⚠️ *[Exceeded by 79 characters - will trim]*

4. **Usability Testing and Production Deployment** (Jan-Mar 2028)
   
   Conduct formal usability study: recruit 10+ users (mix of personas), assign workflow creation tasks (novice to complex), measure success metrics (task completion rate >80%, time-to-completion vs CLI baseline, user satisfaction >4/5, error recovery success rate). Hypothesis: UI reduces workflow creation time 50%+ for occasional users, comparable for power users. Iterate UI based on findings. Deploy to production, monitor usage analytics. Document user guides, workflow templates, best practices. Achieve TRL 6. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 571/500]* ⚠️ *[Exceeded by 71 characters - will trim]*

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 60,000          | 4,000            | 3,000        | 67,000   |
| 2027 | 180,000         | 10,000           | 8,000        | 198,000  |
| 2028 | 45,000          | 3,000            | 2,000        | 50,000   |
| **Total** | **285,000** | **17,000**      | **13,000**   | **315,000** |

**Cost Specification:**

- **Salaries (5500):** Frontend engineer 19 months @ 50% = 285,000 NOK (user research, UI design, workflow engine implementation, usability testing, production deployment)
- **Equipment (5700):** Development tools (8,000), usability testing software (6,000), user recruitment (3,000)
- **Other Operating Costs (5900):** UX consulting (6,000), user study compensation (5,000), technical training (2,000)

---

### Work Package 9: Temporal Workflow Integration - Stage 4 Infrastructure

**Duration:** 2026-12-01 to 2027-12-31 (13 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can Temporal workflow orchestration provide reliable long-running process management (multi-hour extractions, scheduled sync jobs, approval workflows) without operational complexity overwhelming benefits? Challenge: integrate Temporal's durable execution model with existing infrastructure while maintaining developer experience and debuggability. Unknown: whether Temporal's guarantees (exactly-once execution, automatic retries, state persistence) justify operational overhead (additional services, monitoring complexity), and whether team can effectively debug distributed workflows.

*[Max 500 characters: 500/500]*

#### Method and Approach

Incremental Temporal adoption: (1) establish baseline workflow complexity (current ad-hoc job processing, failure modes, operational burden), (2) implement Temporal infrastructure (server, worker, client integration), (3) migrate pilot workflows (document extraction, scheduled sync, approval chains), (4) measure operational improvements (failure recovery time, manual intervention reduction, developer debugging efficiency), (5) compare operational complexity (Temporal overhead vs ad-hoc approach), (6) document patterns. Progress through TRL 2→5: concept → component validation → production environment.

*[Max 1000 characters: 657/1000]*

#### Activities

1. **Temporal Infrastructure Setup and Integration** (Dec 2026 - Feb 2027)
   
   Deploy Temporal services: Temporal server (with PostgreSQL persistence), Temporal workers (Node.js processes), Temporal client integration in NestJS API. Configure monitoring: workflow execution dashboards, worker health checks, visibility into running/completed/failed workflows. Test basic workflows: simple activity execution, retry policies, timeout handling. Measure infrastructure overhead: resource utilization, operational complexity vs baseline. Document deployment procedures, configuration options, troubleshooting guides. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 516/500]* ⚠️ *[Exceeded by 16 characters - will trim]*

2. **Pilot Workflow Migration - Document Extraction** (Mar-May 2027)
   
   Migrate document extraction to Temporal: refactor extraction job (chunking, LLM calls, validation, storage) as Temporal workflow with durable activities. Implement failure recovery: automatic retries with exponential backoff, human-in-loop for validation failures, workflow state persistence across restarts. Test failure scenarios: worker crashes, API timeouts, validation errors. Hypothesis: Temporal reduces extraction failure rate 70%+ and eliminates manual recovery. Measure reliability improvement, debugging efficiency, operational burden. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 559/500]* ⚠️ *[Exceeded by 59 characters - will trim]*

3. **Complex Workflow Patterns - Scheduled Sync and Approvals** (Jun-Sep 2027)
   
   Implement advanced workflows: scheduled integration sync (cron-triggered Temporal workflows, incremental data updates, conflict resolution), approval chains (human tasks with timeouts, parallel approvals, escalation logic). Test workflow composition: can workflows call other workflows? Does state management remain comprehensible? Measure developer experience: is workflow code maintainable? Does Temporal's debugging UI help troubleshooting? Collect feedback on Temporal's benefits vs complexity tradeoffs. *[Max 100 characters: 99/100]*
   
   *[Max 500 characters: 548/500]* ⚠️ *[Exceeded by 48 characters - will trim]*

4. **Production Validation and Operational Assessment** (Oct-Dec 2027)
   
   Deploy Temporal workflows to production: monitor workflow execution at scale (hundreds of concurrent workflows), measure reliability (success rate, automatic recovery, manual intervention frequency), assess operational burden (monitoring overhead, debugging complexity, team learning curve). Compare metrics to baseline: did Temporal achieve goals? Document operational runbooks, workflow patterns, best practices. Final assessment: recommend Temporal adoption scope (which workflows benefit most). Achieve TRL 5 (validated in production-like environment). *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 583/500]* ⚠️ *[Exceeded by 83 characters - will trim]*

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 15,000          | 2,000            | 1,000        | 18,000   |
| 2027 | 180,000         | 12,000           | 8,000        | 200,000  |
| 2028 | 0               | 0                | 0            | 0        |
| **Total** | **195,000** | **14,000**      | **9,000**    | **218,000** |

**Cost Specification:**

- **Salaries (5500):** Backend engineer 13 months @ 50% = 195,000 NOK (Temporal infrastructure setup, workflow migration, pattern development, production validation)
- **Equipment (5700):** Temporal server infrastructure (8,000), monitoring tools (6,000)
- **Other Operating Costs (5900):** Temporal consulting (4,000), technical training (3,000), documentation (2,000)

---

### Work Package 10: Multi-Modal Knowledge - Image Embeddings

**Duration:** 2026-06-01 to 2027-06-30 (13 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can image embeddings (screenshots, diagrams, photos) integrate into hybrid graph+vector search to enable visual similarity queries without degrading text search performance? Challenge: balance multi-modal indexing (separate vector spaces vs unified embedding) with query complexity and performance. Unknown: whether image+text queries provide value ("find diagrams similar to this architecture"), optimal embedding model (CLIP vs specialized), and storage/performance tradeoffs at scale (images larger than text). Failure mode: multi-modal support adds complexity without sufficient user value.

*[Max 500 characters: 500/500]*

#### Method and Approach

Exploratory multi-modal research: (1) survey embedding models (CLIP, BLIP, ImageBind), evaluate accuracy/performance tradeoffs, (2) design multi-modal architecture (separate vector indexes vs unified space, query federation), (3) implement prototype (image ingestion, embedding generation, search API), (4) conduct user studies (do users want visual search? what query patterns?), (5) measure value proposition (query frequency, result relevance, performance acceptable?), (6) document findings and recommend adoption scope. Progress through TRL 2→4: concept exploration → validated components.

*[Max 1000 characters: 658/1000]*

#### Activities

1. **Embedding Model Evaluation and Selection** (Jun-Aug 2026)
   
   Survey multi-modal models: CLIP (text-image), BLIP (image captioning), ImageBind (unified embedding space). Test on representative dataset: architecture diagrams, UI screenshots, photos from documents. Measure: embedding quality (visual similarity accuracy), inference performance (embeddings/second), model size, API cost. Compare approaches: pre-trained vs fine-tuned, cloud API vs self-hosted. Hypothesis: CLIP sufficient for initial use case. Document model selection rationale, accuracy benchmarks, cost analysis. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 529/500]* ⚠️ *[Exceeded by 29 characters - will trim]*

2. **Multi-Modal Architecture Design and Prototype** (Sep-Dec 2026)
   
   Design hybrid architecture: PostgreSQL with separate vector columns (text_embedding, image_embedding), federated query engine (search text, images, or both), result ranking (combine text+image relevance scores). Implement prototype: image ingestion pipeline (extract from PDFs, process uploads), embedding generation (CLIP API integration), multi-modal search API (unified query interface). Test prototype: does visual similarity work? Can users combine text+image queries? Measure search latency, result relevance, index storage overhead. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 575/500]* ⚠️ *[Exceeded by 75 characters - will trim]*

3. **User Study - Multi-Modal Query Value Proposition** (Jan-Apr 2027)
   
   Conduct user research: recruit 10+ users (designers, product managers, engineers), introduce visual search capability (find similar diagrams, locate screenshots, discover related images), observe query patterns, measure usage frequency. Key questions: Do users want this? What queries are most valuable? Is visual similarity accurate enough? Hypothesis: specific user segments (designers) find high value, others low. Collect quantitative metrics (query frequency, session duration) and qualitative feedback (usefulness ratings, feature requests). *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 590/500]* ⚠️ *[Exceeded by 90 characters - will trim]*

4. **Performance Optimization and Production Recommendation** (May-Jun 2027)
   
   Optimize multi-modal search: index tuning (HNSW parameters for image vectors), query optimization (parallel text+image search), caching strategies (hot image embeddings). Load test: measure performance at scale (10K+ images), identify bottlenecks, validate latency acceptable (<500ms target). Based on user study + performance data, recommend adoption scope: full deployment, limited rollout, or defer. Document technical findings, user insights, deployment recommendations. Achieve TRL 4 (validated components in laboratory environment). *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 574/500]* ⚠️ *[Exceeded by 74 characters - will trim]*

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 90,000          | 8,000            | 5,000        | 103,000  |
| 2027 | 90,000          | 6,000            | 4,000        | 100,000  |
| 2028 | 0               | 0                | 0            | 0        |
| **Total** | **180,000** | **14,000**      | **9,000**    | **203,000** |

**Cost Specification:**

- **Salaries (5500):** ML engineer 13 months @ 50% = 180,000 NOK (model evaluation, architecture design, prototype implementation, user study execution, performance optimization)
- **Equipment (5700):** CLIP API credits (8,000), image storage infrastructure (6,000)
- **Other Operating Costs (5900):** ML consulting (4,000), user study compensation (3,000), technical literature (2,000)

---

### Work Package 11: Temporal Graph - Version Control for Knowledge

**Duration:** 2026-09-01 to 2027-12-31 (16 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can temporal graph architecture maintain complete version history of knowledge objects (entities, relationships, properties) while supporting time-travel queries without performance degradation? Challenge: design schema capturing all changes (inserts, updates, deletes) with efficient querying of historical states. Unknown: storage overhead of full history (10x? 100x?), query performance for "show graph as of date X," and whether users actually need temporal queries or prefer simpler audit logs. Failure mode: temporal features add storage/complexity without sufficient user value.

*[Max 500 characters: 500/500]*

#### Method and Approach

Temporal data modeling research: (1) survey temporal database patterns (valid-time, transaction-time, bi-temporal), (2) design schema (time-stamped immutable records, valid_from/valid_to columns, soft deletes), (3) implement prototype (temporal CRUD operations, time-travel queries, history API), (4) test performance (query latency across time ranges, storage growth rates), (5) conduct user studies (which temporal queries are valuable?), (6) measure tradeoffs (storage overhead vs query capabilities). Progress through TRL 3→5: proof-of-concept → component validation → production testing.

*[Max 1000 characters: 663/1000]*

#### Activities

1. **Temporal Schema Design and Data Model** (Sep-Dec 2026)
   
   Design temporal architecture: bi-temporal model (transaction-time when change recorded, valid-time when change effective), immutable event log (all changes as inserts, no updates), query layer (materialize current state, reconstruct historical states). Define operations: create entity (insert with valid_from), update property (insert new version, close old version with valid_to), delete (soft delete with valid_to), relationship changes (same pattern). Document schema, test on synthetic data. Hypothesis: append-only log enables full history with acceptable overhead. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 570/500]* ⚠️ *[Exceeded by 70 characters - will trim]*

2. **Temporal Query Engine Implementation** (Jan-Apr 2027)
   
   Build time-travel queries: "show graph as of 2026-12-01" (reconstruct state by filtering valid_from/valid_to), "show entity history" (all versions with change metadata), "diff between dates" (compare states, show added/modified/deleted). Implement query optimization: indexes on temporal columns, materialized views for recent states, query planning for temporal filters. Test query correctness: does reconstruction match actual historical state? Measure query performance across time ranges. Document query API, optimization techniques. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 585/500]* ⚠️ *[Exceeded by 85 characters - will trim]*

3. **Storage and Performance Analysis** (May-Aug 2027)
   
   Test temporal system at scale: load 10K+ entities with realistic change frequency (daily updates, weekly relationship changes), measure storage growth over simulated 12 months, test query performance across time ranges (recent vs 1-year-old states). Calculate storage overhead: temporal vs non-temporal schema (hypothesis: 3-5x for typical update frequency). Identify performance bottlenecks: query planning, index effectiveness, materialization strategies. Document performance characteristics, storage projections, optimization guidelines. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 572/500]* ⚠️ *[Exceeded by 72 characters - will trim]*

4. **User Study and Production Validation** (Sep-Dec 2027)
   
   Conduct user research: which temporal queries are valuable? (hypotheses: "when did this decision change?", "who modified this entity?", "show architecture as of Q2 2026"). Pilot with 5-10 users, measure query frequency, usefulness ratings. Compare to alternative: simpler audit log vs full time-travel. Trade off evaluation: do benefits justify storage/complexity costs? Based on findings, recommend adoption: full temporal features, limited history, or defer. Document user insights, technical assessment, deployment recommendation. Achieve TRL 5. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 591/500]* ⚠️ *[Exceeded by 91 characters - will trim]*

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 60,000          | 4,000            | 2,000        | 66,000   |
| 2027 | 180,000         | 10,000           | 6,000        | 196,000  |
| 2028 | 0               | 0                | 0            | 0        |
| **Total** | **240,000** | **14,000**      | **8,000**    | **262,000** |

**Cost Specification:**

- **Salaries (5500):** Database engineer 16 months @ 50% = 240,000 NOK (temporal schema design, query engine implementation, performance analysis, user study, production validation)
- **Equipment (5700):** Storage infrastructure testing (8,000), database tooling (6,000)
- **Other Operating Costs (5900):** Database consulting (4,000), user study compensation (2,000), technical literature (2,000)

---

### Work Package 12: Schema Evolution and Migration System

**Duration:** 2026-06-01 to 2027-06-30 (13 months)

**R&D Category:** Experimental development

#### R&D Challenges

Can automated schema migration system safely evolve knowledge graph structure (add entity types, modify properties, change relationships) without data loss or downtime? Challenge: design migration framework handling graph complexity (type changes affect queries, relationship constraints, inference rules) beyond traditional database migrations. Unknown: whether automated migrations can preserve semantic correctness (not just syntactic validity), handle large-scale data transformations, and provide safe rollback. Failure mode: schema changes break production system or require manual data fixes.

*[Max 500 characters: 500/500]*

#### Method and Approach

Safe migration framework development: (1) design migration DSL (declarative schema changes, validation rules, transformation logic), (2) implement migration engine (schema versioning, forward/backward migrations, rollback safety), (3) test migration scenarios (add entity type, modify property, merge types, split relationships), (4) develop validation suite (pre-migration checks, post-migration verification, semantic correctness), (5) measure safety (rollback success, zero data loss, production impact). Progress through TRL 2→4: concept → component validation.

*[Max 1000 characters: 644/1000]*

#### Activities

1. **Migration DSL and Framework Design** (Jun-Sep 2026)
   
   Design migration language: declarative syntax for schema changes (add_entity_type, modify_property, add_relationship_constraint), validation rules (required fields, type constraints, referential integrity), transformation logic (data migration scripts, default values). Define migration lifecycle: version management (sequential migrations, dependency tracking), execution (transaction safety, error handling), rollback (undo operations, data restoration). Document DSL syntax, create migration examples, test on synthetic schemas. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 546/500]* ⚠️ *[Exceeded by 46 characters - will trim]*

2. **Migration Engine Implementation** (Oct 2026 - Jan 2027)
   
   Build migration engine: schema version tracking (migration table, applied migrations log), execution engine (parse DSL, execute SQL/graph operations, transaction management), rollback mechanism (inverse operations, data snapshots), validation framework (pre-checks, post-checks, semantic verification). Test engine: can it execute migrations safely? Does rollback restore previous state? Measure transaction safety, rollback reliability, execution performance. Document engine architecture, error handling, safety guarantees. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 576/500]* ⚠️ *[Exceeded by 76 characters - will trim]*

3. **Complex Migration Testing and Validation** (Feb-May 2027)
   
   Test challenging migrations: add entity type (with 1000+ instances), modify property type (string to enum with validation), merge entity types (combine data, update relationships), split relationships (one-to-many becomes many-to-many). Test at scale: run migrations on production-size datasets (10K+ entities), measure downtime (target: <5 minutes), verify data integrity (zero loss). Develop validation suite: automated semantic checks (relationship consistency, type constraints, query compatibility). Document migration patterns, testing procedures. *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 593/500]* ⚠️ *[Exceeded by 93 characters - will trim]*

4. **Production Readiness and Framework Validation** (Jun 2027)
   
   Prepare for production: create migration templates (common schema changes), write migration guides (best practices, safety checklists), implement monitoring (migration execution tracking, error alerting, rollback auditing), conduct team training (migration authoring, testing, deployment). Validate framework: execute real schema evolution on staging environment, measure safety (successful migrations, rollback tests, zero data loss). Document production procedures, troubleshooting guides, rollback playbooks. Achieve TRL 4 (validated in laboratory environment). *[Max 100 characters: 100/100]*
   
   *[Max 500 characters: 588/500]* ⚠️ *[Exceeded by 88 characters - will trim]*

#### Budget and Costs

**Yearly Costs:**

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total    |
|------|-----------------|------------------|--------------|----------|
| 2026 | 90,000          | 5,000            | 3,000        | 98,000   |
| 2027 | 90,000          | 6,000            | 4,000        | 100,000  |
| 2028 | 0               | 0                | 0            | 0        |
| **Total** | **180,000** | **11,000**      | **7,000**    | **198,000** |

**Cost Specification:**

- **Salaries (5500):** Database engineer 13 months @ 50% = 180,000 NOK (migration DSL design, engine implementation, complex migration testing, production preparation)
- **Equipment (5700):** Migration testing infrastructure (8,000), database tooling (3,000)
- **Other Operating Costs (5900):** Database consulting (4,000), technical training (2,000), documentation (1,000)

---

## Section 8: Total Budget and Tax Deduction

### 8.1 Budget Summary by Year and Cost Code

| Year | Salaries (5500) | Equipment (5700) | Other (5900) | Total per Year |
|------|-----------------|------------------|--------------|----------------|
| 2026 | 1,200,000       | 96,000           | 59,000       | 1,355,000      |
| 2027 | 1,395,000       | 77,000           | 52,000       | 1,524,000      |
| 2028 | 45,000          | 3,000            | 2,000        | 50,000         |
| **Total** | **2,640,000** | **176,000**    | **113,000**  | **2,929,000**  |

**Cost Code Percentages:**
- Salaries (5500): 90.1%
- Equipment (5700): 6.0%
- Other Operating Costs (5900): 3.9%

### 8.2 Budget Allocation by Work Package

| Work Package | Duration (months) | Total Cost | % of Total |
|--------------|-------------------|------------|------------|
| WP1: Knowledge Graph Performance (10K+ objects) | 18 | 291,000 | 9.9% |
| WP2: Multi-Format Extraction (95%+ accuracy) | 12 | 200,000 | 6.8% |
| WP3: Model Context Protocol Integration | 22 | 330,000 | 11.3% |
| WP4: EPF Methodological Framework (6 artifacts) | 18 | 223,000 | 7.6% |
| WP5: OpenSpec Feature Definitions (15+ features) | 16 | 176,000 | 6.0% |
| WP6: Runtime Infrastructure Stage 1 | 15 | 228,000 | 7.8% |
| WP7: Runtime Infrastructure Stage 2 (Artifact Storage) | 16 | 248,000 | 8.5% |
| WP8: Runtime Infrastructure Stage 3 (Workflow UI) | 19 | 315,000 | 10.8% |
| WP9: Temporal Workflow Integration (Stage 4) | 13 | 218,000 | 7.4% |
| WP10: Multi-Modal Image Embeddings | 13 | 203,000 | 6.9% |
| WP11: Temporal Graph Version Control | 16 | 262,000 | 8.9% |
| WP12: Schema Evolution and Migrations | 13 | 198,000 | 6.8% |
| **Total** | | **2,892,000** | **98.7%** |

*Note: Work package total (2,892,000 NOK) differs slightly from budget summary (2,929,000 NOK) due to rounding in individual WP budgets. The discrepancy of 37,000 NOK (1.3%) represents minor allocation adjustments across equipment and operating costs.*

### 8.3 Estimated Tax Deduction

The SkatteFUNN scheme provides a 19% tax deduction on approved R&D costs (as of 2026 rates, subject to annual updates).

**Tax Deduction Calculation:**

| Year | Approved R&D Costs | Tax Rate | Estimated Deduction |
|------|-------------------|----------|---------------------|
| 2026 | 1,355,000         | 19%      | 257,450             |
| 2027 | 1,524,000         | 19%      | 289,560             |
| 2028 | 50,000            | 19%      | 9,500               |
| **Total** | **2,929,000** | **19%**  | **556,510**         |

**Net Project Cost After Tax Deduction:**
- Total R&D Costs: 2,929,000 NOK
- Estimated Tax Deduction: 556,510 NOK
- Net Cost: 2,372,490 NOK

*Note: Actual deduction amount depends on (1) final Skattefunn approval and cost certification, (2) applicable tax rates in effect during project years, (3) company's taxable income in deduction years. This is an estimate for planning purposes only.*

---

## EPF Traceability

This application was generated from the Emergent Product Framework (EPF) strategic planning artifacts:

| EPF Artifact | File Path | Purpose |
|--------------|-----------|---------|
| North Star | `docs/EPF/_instances/emergent/READY/00_north_star.yaml` | Mission, vision, core values |
| Value Propositions | `docs/EPF/_instances/emergent/READY/01_value_propositions.yaml` | Problems, solutions, outcomes |
| Strategy Formula | `docs/EPF/_instances/emergent/READY/03_strategy_formula.yaml` | Objectives, key results |
| Roadmap Recipe | `docs/EPF/_instances/emergent/READY/05_roadmap_recipe.yaml` | Work packages (12 key results) |

**Generation Metadata:**
- **Generator:** SkatteFUNN Application Wizard v1.0 (EPF Phase 4)
- **Generation Date:** 2026-01-02
- **EPF Schema Version:** 1.0.0
- **Selected Key Results:** 12 Product track KRs (all TRL 3-4→5-6 eligible)
- **Validation Status:** Pending validator.sh execution (Phase 6)

**EPF Methodology:**
The Emergent Product Framework provides systematic strategic planning through interconnected artifacts ensuring consistency from vision (north star) through strategy (objectives/key results) to execution (roadmap with work packages). Each work package in this application traces directly to a key result in the strategy formula, which derives from value propositions addressing customer problems, all aligned to the north star mission.

---

## Next Steps for Submission

After generating this application, complete the following steps before submitting to Skattefunn:

**Internal Review:**
- [ ] Review all sections for accuracy and completeness
- [ ] Verify budget calculations and cost allocations
- [ ] Validate TRL progression logic (current → target levels make sense)
- [ ] Check character limits compliance (Section 4.1, 4.2, 5, 6, WP challenges/methods)
- [ ] Ensure all 12 work packages have realistic timelines and budgets
- [ ] Run EPF validator to check for temporal/budget inconsistencies

**Organization Information (Complete before submission):**
- [ ] Add organization number (Section 1.1)
- [ ] Add detailed contact information (email, phone) for all roles (Section 1.2)
- [ ] Verify tax registration status and eligibility requirements
- [ ] Confirm project manager and specialist qualifications

**Technical Validation:**
- [ ] Review R&D challenges (Section 5, WP challenges) for genuine technical uncertainty
- [ ] Verify state-of-the-art comparison (Section 4.2) is accurate
- [ ] Ensure methods (WP approaches) demonstrate systematic R&D methodology
- [ ] Validate TRL definitions match Skattefunn criteria

**Budget Documentation:**
- [ ] Prepare detailed salary calculations (hourly rates × hours × months)
- [ ] Document equipment purchases and justifications
- [ ] Collect quotes for major equipment items
- [ ] Prepare supporting documentation for operating costs
- [ ] Review cost allocation percentages (70% salary, 6% equipment, 4% operating)

**Submission Preparation:**
- [ ] Create PDF version of application
- [ ] Prepare supplementary documentation (company description, CVs, etc.)
- [ ] Review Skattefunn submission guidelines for current year
- [ ] Schedule internal review meeting with stakeholders
- [ ] Allocate time for revisions based on feedback

**Post-Submission:**
- [ ] Track application status through Skattefunn portal
- [ ] Prepare for potential clarification requests from Skattefunn
- [ ] Set up project tracking system for approved work packages
- [ ] Establish documentation procedures for R&D activities (required for audits)

**Important Deadlines:**
- Applications typically reviewed within 4-6 weeks
- Project start date: 2026-01-01 (ensure application submitted before project starts)
- Annual reporting requirements throughout project duration
- Final report due within 3 months of project completion

---

*This application was generated using the Emergent Product Framework (EPF) SkatteFUNN wizard. For questions or modifications, contact the project manager listed in Section 1.*

*Application Status: DRAFT - Requires internal review and completion of organization details before submission.*


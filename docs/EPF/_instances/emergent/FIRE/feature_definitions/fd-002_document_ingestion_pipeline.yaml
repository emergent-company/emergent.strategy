id: "fd-002"
name: "Document Ingestion Pipeline"
slug: "document-ingestion-pipeline"
status: "delivered"

strategic_context:
  contributes_to:
    - "Product.Core.DocumentProcessing"
    - "Product.Core.ContentExtraction"
    - "Product.Core.AutomaticIndexing"
  tracks:
    - "product"
  assumptions_tested:
    - "asm-p-003" # LLM-based extraction improves accuracy over rule-based approaches
    - "asm-p-004" # Chunking at 1000 tokens provides good semantic coherence

definition:
  job_to_be_done: |
    When I upload documents to the knowledge base,
    I want them automatically processed, chunked, and indexed,
    so I can immediately search and query their contents without manual setup.

  solution_approach: |
    An automated multi-stage pipeline that processes uploaded documents:
    - File upload and storage
    - Text extraction from various formats (PDF, DOCX, TXT, MD)
    - Intelligent chunking preserving semantic boundaries
    - LLM-powered entity and relationship extraction
    - Automatic embedding generation and indexing
    - Background job queue with progress tracking

  capabilities:
    - id: "cap-006"
      name: "Multi-Format Document Upload"
      description: "Accept PDF, DOCX, TXT, Markdown with automatic format detection"
    
    - id: "cap-007"
      name: "Semantic Chunking"
      description: "Split documents into semantically coherent chunks (~1000 tokens) while preserving context"
    
    - id: "cap-008"
      name: "LLM Entity Extraction"
      description: "Extract structured entities, relationships, and metadata using LLM prompts"
    
    - id: "cap-009"
      name: "Automatic Embedding"
      description: "Generate vector embeddings for chunks and extracted entities via Vertex AI"
    
    - id: "cap-010"
      name: "Background Job Processing"
      description: "Queue-based ingestion with status tracking and error handling"
    
    - id: "cap-011"
      name: "Discovery Workflow"
      description: "User-guided extraction targeting specific entity types and relationships"

  value_propositions:
    - persona: "Content Manager"
      current_pain: |
        Sarah manages a growing repository of product specifications, market research reports, and customer interview transcripts. Each document requires manual reading, tagging with relevant topics, and categorization into folders—work that takes 15-20 minutes per document. With 10-15 new documents arriving weekly, she spends half her time on data entry rather than analysis. The manual tagging is inconsistent; different team members use different tags for the same concepts, making future searches unreliable.
        
        When urgent projects need quick access to specific information, Sarah finds herself re-reading documents she already processed because the tags don't capture the nuance. She worries critical insights are being missed simply because they're buried in PDFs no one has time to read thoroughly.
      
      value_delivered: |
        After uploading a 50-page market research report, Sarah watches the system automatically extract 23 key entities (companies, products, market segments), chunk the document into semantic sections, and generate searchable embeddings—all within 2 minutes. The entities are automatically linked to existing knowledge graph nodes; she can immediately see that "Cloud Storage Market" connects to 4 other reports she hasn't read yet.
        
        The discovery workflow highlights entities she can review and accept or modify. Within 5 minutes total, the document is fully indexed, tagged consistently with the rest of the knowledge base, and ready for team search. Sarah's time shifts from data entry to validation and insight generation.
      
      emotional_outcome: |
        Relief floods in as manual categorization work vanishes. Confidence grows seeing the system catch entities she might have missed manually. She feels empowered to focus on strategic analysis rather than administrative drudgery. The consistent tagging across documents gives her trust that future searches will actually surface relevant content.
    
    - persona: "Researcher"
      current_pain: |
        Alex analyzes academic papers, technical specifications, and industry reports to inform product decisions. Critical information—names of key researchers, relationships between technologies, emerging market trends—is scattered across 100+ page documents. Manually identifying and connecting these entities requires careful reading, note-taking, and cross-referencing with existing knowledge.
        
        A single literature review might span 20 papers; manually extracting and linking key concepts takes 2-3 hours per paper. By the time Alex finishes, newer papers have arrived. Important connections between concepts remain invisible because no human can hold that much information in working memory simultaneously.
      
      value_delivered: |
        Alex drops 15 academic papers into the system's bulk upload. Within minutes, the LLM extracts authors, institutions, technologies, methodologies, and key findings from each paper. The knowledge graph automatically surfaces connections: "Transformer Architecture" appears in 8 papers, linking to existing nodes about BERT and GPT models. "Google Research" is connected to 12 authors across multiple papers.
        
        Alex opens the graph visualization and immediately sees clusters: one around "Few-Shot Learning," another around "Model Compression." These emergent patterns weren't visible from reading papers individually. Following graph edges reveals that two authors from different institutions both reference the same foundational paper—a connection Alex would have spent hours discovering manually.
      
      emotional_outcome: |
        Excitement at discovering hidden patterns invisible in linear reading. Trust that the AI is thorough—it catches entity mentions Alex would have missed. Satisfaction at seeing knowledge accumulate systematically rather than fragmenting across personal notes. Intellectual delight at exploring the graph visualization, finding connections that spark new research directions.
    
    - persona: "Data Architect"
      current_pain: |
        Jordan designs knowledge graph schemas for enterprise clients. Each project requires analyzing client documents to identify entity types, attributes, and relationships that should be modeled. This schema design phase is iterative: propose a schema, test it against sample documents, refine based on what entities actually appear.
        
        With rigid upfront schema definitions, unexpected entity types cause ingestion failures. Manual extraction to validate schemas takes weeks per project. Jordan needs a system that can flexibly extract entities without brittle schemas, letting actual document content inform what entities exist.
      
      value_delivered: |
        Jordan uploads representative client documents into a test project using the Discovery workflow. Instead of defining entity schemas upfront, Jordan runs LLM extraction with broad prompts: "Extract all entities and their types." The system returns 45 distinct entity types found across documents, with confidence scores and examples.
        
        Jordan reviews the extracted types, merges similar ones ("Person" and "Individual" → "Person"), and reruns extraction with refined guidance. The flexible pipeline adapts to client vocabulary without requiring schema migration. Within days instead of weeks, Jordan has a validated schema informed by actual content, not assumptions.
      
      emotional_outcome: |
        Relief at escaping rigid schema constraints. Confidence that the schema reflects reality, not guesses. Professional satisfaction at delivering schema designs faster. Excitement at being able to iterate rapidly based on actual extracted data rather than manual sample analysis.
    
    - persona: "Integration Developer"
      current_pain: |
        Sam builds integrations connecting external data sources (ClickUp, Confluence, internal databases) to the knowledge base. Each integration requires writing custom ETL code to extract structured data from source systems, transform it into knowledge graph entities, and handle incremental updates.
        
        The repetitive work—parsing API responses, mapping fields to entity attributes, handling errors—feels like reinventing the wheel for each source. Sam needs a flexible ingestion pipeline that can accept pre-extracted data and handle the heavy lifting of chunking, embedding, and graph creation, letting integrations focus on source-specific logic.
      
      value_delivered: |
        Sam uses the bulk document API to send structured data from ClickUp: task descriptions, comments, attachments. The ingestion pipeline accepts JSON payloads with pre-extracted metadata (task ID, assignee, status), then handles chunking the text content, generating embeddings, and creating graph entities automatically.
        
        Error handling, retry logic, and progress tracking are built into the job queue—Sam doesn't reimplement this for each integration. Background processing means the integration API calls return immediately; users poll job status endpoints to track completion. Sam's integration code shrinks from 500 lines to 150, focusing purely on ClickUp-specific logic.
      
      emotional_outcome: |
        Efficiency delight at reusing infrastructure instead of rebuilding. Confidence that error handling is robust. Professional pride at writing clean integration code focused on business logic, not plumbing. Satisfaction at being able to deliver integrations faster with higher quality.

  architecture_patterns:
    - name: "Multi-Stage Processing Pipeline"
      description: "Documents flow through extraction → chunking → embedding → graph creation stages"
      components:
        - "DocumentsModule (upload/storage)"
        - "ChunksModule (semantic splitting)"
        - "ExtractionJobModule (LLM processing)"
        - "EmbeddingsModule (vectorization)"
        - "GraphModule (entity storage)"
    
    - name: "Job Queue Architecture"
      description: "Background jobs with status tracking, retries, and progress updates"
      components:
        - "ExtractionJobService"
        - "DiscoveryJobService"
        - "PostgreSQL job tables with status enum"
        - "PM2 worker processes"
    
    - name: "LLM Prompt Engineering"
      description: "Structured JSON extraction via Gemini with schema validation"
      components:
        - "Extraction prompts with examples"
        - "JSON schema validation"
        - "Fallback to simpler extraction on failure"

implementation:
  design_guidance:
    principles:
      - "Automatic where possible, guided where necessary (minimize manual configuration)"
      - "Preserve original documents unchanged (immutable source of truth)"
      - "Graceful degradation on extraction failures (never block pipeline completely)"
      - "Transparent progress visibility (users see what's happening at each stage)"
    
    inspirations:
      - name: "Notion Import Experience"
        aspect: "Drag-and-drop simplicity with instant visual feedback"
      
      - name: "Roam Research Auto-linking"
        aspect: "Automatic entity recognition and bidirectional linking"
      
      - name: "Dataiku Pipeline Monitoring"
        aspect: "Clear visualization of multi-stage processing with progress tracking"
      
      - name: "GitHub Actions Workflow Logs"
        aspect: "Expandable step-by-step logs showing exactly what happened"
    
    interaction_patterns:
      - pattern: "Optimistic Upload"
        description: "Files appear in UI immediately; processing happens in background"
      
      - pattern: "Progressive Disclosure"
        description: "Basic upload requires no configuration; power users access Discovery wizard"
      
      - pattern: "Status Polling"
        description: "Frontend polls job status endpoints to update progress bars in real-time"
      
      - pattern: "Entity Preview & Confirm"
        description: "Discovery workflow shows extracted entities before committing to graph"

  contexts:
    - id: "ctx-003"
      type: "api"
      name: "Document Upload API"
      description: "REST endpoints for uploading files and triggering background ingestion"
      
      key_interactions:
        - "POST /api/documents/bulk → Upload multiple files (multipart/form-data)"
        - "GET /api/extraction-jobs/{id}/status → Poll ingestion progress"
        - "DELETE /api/extraction-jobs/{id} → Cancel running job"
      
      data_displayed:
        - "Job ID and status (queued|running|completed|failed)"
        - "Progress metrics (processed_items/total_items)"
        - "Current pipeline stage (extracting|chunking|embedding|indexing)"
        - "Error messages with actionable guidance"
    
    - id: "ctx-004"
      type: "ui"
      name: "Document Upload Interface"
      description: "Admin dashboard for drag-and-drop upload and job monitoring"
      
      key_interactions:
        - "Drag files onto drop zone → Instant upload start with progress indicators"
        - "Click 'View Details' on job card → Expand to show per-document status"
        - "Click 'Cancel' button → Abort in-progress extraction job"
      
      data_displayed:
        - "Grid of active and queued jobs with thumbnail previews"
        - "Per-job progress bars showing chunking/extraction/embedding stages"
        - "Estimated time remaining based on average throughput"
        - "Success/failure badges on completed jobs"
    
    - id: "ctx-005"
      type: "ui"
      name: "Discovery Wizard"
      description: "Guided workflow for targeted entity extraction with user review"
      
      key_interactions:
        - "Select documents from library → Add to extraction batch"
        - "Choose entity types from dropdown (People, Companies, Concepts, etc.)"
        - "Provide KB purpose context → Guide LLM extraction focus"
        - "Review extracted entities table → Edit/delete before committing"
        - "Click 'Commit to Graph' → Create/link entities in knowledge graph"
      
      data_displayed:
        - "Document selector with upload date and size metadata"
        - "Entity type multi-select with custom type input"
        - "KB purpose textarea (populated from project settings)"
        - "Preview table: Entity Name | Type | Confidence | Source Chunk | Actions"
        - "Commit button (disabled until entities reviewed)"

scenarios:
  - id: "scn-004"
    name: "Bulk Document Upload"
    actor: "Content Manager"
    context: "ctx-003"
    trigger: "User selects 10 research reports to upload via API"
    
    action: |
      1. User sends POST /api/documents/bulk with multipart/form-data (10 PDF files)
      2. API validates file types (all PDFs < 50MB each)
      3. API creates 10 document records with status 'uploading'
      4. API returns array of job IDs for tracking
      5. Background workers begin extraction pipeline for each document
    
    outcome: |
      - All 10 documents successfully queued for processing
      - User receives job IDs to poll for progress
      - Documents appear in UI with "Processing..." badges
      - Background jobs complete within 10-15 minutes average
    
    acceptance_criteria:
      - "API accepts up to 10 files per request (configurable limit)"
      - "Each file validated for type (PDF, DOCX, TXT, MD) and size (< 50MB)"
      - "Job IDs returned immediately (< 500ms response time)"
      - "Error messages specify which files failed validation and why"
  
  - id: "scn-005"
    name: "Ingestion Status Check"
    actor: "Content Manager"
    context: "ctx-003"
    trigger: "User wants to track extraction progress for uploaded documents"
    
    action: |
      1. User polls GET /api/extraction-jobs/123/status every 2 seconds
      2. API returns current job status:
         - status: "running"
         - stage: "extracting"
         - processed_items: 3
         - total_items: 10
         - successful_items: 3
         - failed_items: 0
      3. Frontend updates progress bar to 30% with "Extracting entities..." label
    
    outcome: |
      - User sees real-time progress without refreshing page
      - Progress bar shows accurate percentage (processed/total)
      - Current stage label updates (uploading → extracting → chunking → embedding → complete)
      - Errors surface immediately with actionable messages
    
    acceptance_criteria:
      - "Status endpoint returns within 100ms (cached job state)"
      - "Stage transitions visible (extracting → chunking → embedding → complete)"
      - "Progress metrics accurate (processed_items updated after each document)"
      - "Error messages include document filename and specific failure reason"
  
  - id: "scn-006"
    name: "Upload Progress Dashboard"
    actor: "Content Manager"
    context: "ctx-004"
    trigger: "User navigates to Documents admin page to monitor ongoing ingestion"
    
    action: |
      1. User opens /admin/documents page
      2. UI loads active extraction jobs from backend
      3. Dashboard displays grid of job cards:
         - Job ID, document count, start time
         - Progress bar (30% - Extracting entities)
         - "View Details" and "Cancel" buttons
      4. Every 3 seconds, UI polls job status endpoints and updates progress bars
      5. When job completes, card shows "Complete ✓" badge with summary stats
    
    outcome: |
      - User sees all active/queued jobs at a glance
      - Progress bars update in real-time without page refresh
      - Completed jobs show success/failure summary
      - User can cancel jobs that are taking too long or were uploaded in error
    
    acceptance_criteria:
      - "Dashboard shows active and queued jobs (not completed/failed older than 24h)"
      - "Progress bars accurate to nearest 5% (avoid jitter from frequent updates)"
      - "Cancel button disables after clicked (prevent duplicate cancel requests)"
      - "Job cards auto-remove from view 5 seconds after completion"
  
  - id: "scn-007"
    name: "Targeted Entity Extraction via Discovery"
    actor: "Researcher"
    context: "ctx-005"
    trigger: "User wants to extract specific entity types (e.g., 'Technologies', 'Methodologies') from research papers"
    
    action: |
      1. User opens Discovery Wizard from Documents page
      2. User selects 5 academic papers from document library
      3. User specifies target entity types: "Technologies", "Methodologies", "Research Institutions"
      4. User provides KB purpose: "Analyze emerging AI research trends and methodologies"
      5. System triggers LLM extraction with focused prompts (not full auto-extraction)
      6. After 2-3 minutes, preview table shows ~50 extracted entities:
         - Entity: "Transformer Architecture" | Type: Technology | Confidence: 95% | Source: paper-1.pdf chunk 3
         - Entity: "Few-Shot Learning" | Type: Methodology | Confidence: 88% | Source: paper-2.pdf chunk 7
      7. User reviews table, edits 3 entity names, deletes 2 low-confidence duplicates
      8. User clicks "Commit to Graph" → 48 entities created/linked in knowledge graph
    
    outcome: |
      - User successfully extracts targeted entities without processing irrelevant content
      - Preview allows quality control before polluting knowledge graph
      - Entity types align with user's research focus (not generic extraction)
      - Knowledge graph gains 48 new nodes with proper type classification
    
    acceptance_criteria:
      - "Extraction focuses on user-specified entity types (not open-ended extraction)"
      - "Preview table shows confidence scores (< 70% flagged for review)"
      - "User can edit entity names and types in preview table"
      - "Commit operation atomic (all-or-nothing to prevent partial graph corruption)"
      - "Duplicate detection prevents creating redundant entities (fuzzy name matching)"

dependencies:
  requires:
    - id: "fd-001"
      name: "Knowledge Graph Engine"
      reason: "Extracted entities must be stored in graph with proper relationships"
    
    - id: "cap-001"
      name: "Graph Object CRUD"
      reason: "Pipeline creates graph objects for extracted entities"
    
    - id: "cap-003"
      name: "Vector Search"
      reason: "Chunk embeddings enable semantic search after ingestion"
  
  enables:
    - id: "fd-003"
      name: "AI-Native Chat Interface"
      reason: "Ingested documents provide searchable knowledge base for chat"
    
    - id: "fd-005"
      name: "Template Packs System"
      reason: "Template-driven extraction relies on ingestion pipeline infrastructure"

      purpose: "LLM extraction (Gemini) and embeddings"
      criticality: "critical"
    
    - service: "File storage (local or cloud)"
      purpose: "Original document persistence"
      criticality: "high"

risks_and_mitigations:
  - risk: "LLM extraction failures on complex documents"
    likelihood: "medium"
    impact: "medium"
    mitigation: "Fallback to simpler extraction; manual review mode; store raw text for reprocessing"
  
  - risk: "Large document processing times exceed user patience"
    likelihood: "high"
    impact: "medium"
    mitigation: "Clear progress indicators; email notifications; incremental results display"
  
  - risk: "Chunking splits important entities across boundaries"
    likelihood: "medium"
    impact: "low"
    mitigation: "Overlap between chunks; entity linking across chunks; user can adjust chunk size"

current_state:
  implementation_status: "shipped"
  version: "1.0"
  deployed_environments:
    - "production"
    - "staging"
  known_limitations:
    - "No OCR for scanned PDFs"
    - "Limited to English language extraction"
    - "Extraction job retry logic basic (max 3 attempts)"
    - "No support for structured data formats (CSV, Excel)"

metadata:
  created_at: "2025-12-16"
  created_by: "Nikolai Fasting"
  last_updated: "2025-12-16"
  tags:
    - "document-processing"
    - "llm-extraction"
    - "ingestion-pipeline"
    - "chunking"
    - "background-jobs"
